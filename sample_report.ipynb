{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model for Costa Reican Poverty Level Predicion\n",
    "\n",
    "### Outline\n",
    "**1. Project Setup** \\\n",
    "*1.1 Load Data and Packages* \\\n",
    "*1.2 Data Cleaning* \\\n",
    "\\\n",
    "**2. What is a Naive Bayes Model?** \\\n",
    "*2.1 Summary of Approach* \\\n",
    "*2.2 Summary of Findings* \\\n",
    "\\\n",
    "**3. Models** \\\n",
    "*3.1 Basic Models* \\\n",
    "*3.2 Improving Model Performance* \\\n",
    "\\\n",
    "**4. Limitation and Next Steps**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discuss with team:\n",
    "- what features are not being included - is this model including all features? do we need an updated data dict w some of the generated features?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathryn/Library/Caches/pypoetry/virtualenvs/costa-rican-household-poverty-level-predic-Bkx66hd9-py3.11/lib/python3.11/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, recall_score, classification_report\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "import load_data as ld\n",
    "import numpy as np\n",
    "df, X_valid, y_valid = ld.load_train_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Id', 'idhogar', 'rez_esc']\n",
    "\n",
    "for dfs in [df, X_valid, y_valid]:\n",
    "    dfs.replace([np.inf, -np.inf], np.nan, inplace=True) # TO DO: Fix upstream to deal with inf values\n",
    "    for col in cols_to_drop:\n",
    "        if col in dfs.columns:\n",
    "            dfs.drop(col, axis=1, inplace=True)\n",
    "    dfs.fillna(df.mean(), inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X and y for training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.loc[:, 'Target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper to Print Accuracy Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = False):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    if cm is True:\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary of Datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'rez_esc' not in Gaussian, 'parentesco1' not in bernoulli\n",
    "nb_datatypes = {'gaussian' :['agesq', 'dependency' , 'edjefa', 'edjefe', 'escolari', 'tamhog'], \n",
    "                'bernoulli' : ['pisomoscer', 'paredzocalo', 'techoentrepiso', 'elimbasu5', 'tipovivi5', 'tipovivi2', \n",
    "                               'instlevel5', 'instlevel3', 'instlevel7', 'coopele', 'planpri', 'v14a', 'parentesco9', \n",
    "                               'parentesco10', 'dis', 'estadocivil4', 'parentesco8', 'female', 'eviv1', 'eviv3', 'eviv2', \n",
    "                               'estadocivil2', 'parentesco6', 'estadocivil1', 'male', 'estadocivil3', 'mobilephone', \n",
    "                               'parentesco7', 'pisonotiene', 'abastaguano', 'parentesco11', 'parentesco12', 'pisonatur', 'pisocemento', \n",
    "                               'pisoother', 'pisomadera', 'paredblolad', 'paredfibras', 'paredother', 'paredpreb', 'pareddes', 'paredmad', \n",
    "                               'paredzinc', 'techozinc', 'techocane', 'techootro', 'etecho1', 'etecho3', 'etecho2', 'elimbasu2', 'elimbasu3', \n",
    "                               'elimbasu1', 'elimbasu4', 'elimbasu6', 'estadocivil5', 'estadocivil7', 'parentesco3', 'parentesco5', \n",
    "                               'parentesco2', 'parentesco4', 'cielorazo', 'computer', 'refrig', 'television', 'epared1', 'epared3', \n",
    "                               'epared2', 'abastaguadentro', 'abastaguafuera', 'estadocivil6', 'instlevel4', 'instlevel2', 'instlevel6', \n",
    "                               'energcocinar2', 'energcocinar3', 'energcocinar4', 'noelec', 'instlevel1', 'energcocinar1', 'sanitario1', \n",
    "                               'hacdor', 'hacapo', 'tipovivi1', 'instlevel9', 'tipovivi4', 'lugar4', 'lugar1', 'lugar2', 'lugar5', 'lugar6', \n",
    "                               'lugar3', 'tipovivi3', 'sanitario3', 'sanitario5', 'sanitario6', 'sanitario2', 'instlevel8', 'area1', 'v18q', 'area2'],\n",
    "                               'multinomial' : [\"r4m2\", \"r4m1\", \"hhsize\", \"r4h2\", \"r4h1\", \"hogar_nin\", \"tamviv\", \"v18q1\", \"r4t2\", \"r4t1\", \n",
    "                                \"r4m3\", \"r4h3\", \"r4t3\", \"meaneduc\", \"qmobilephone\", \"hogar_total\", \"overcrowding\", \"hogar_mayor\", \n",
    "                                \"bedrooms\", \"hogar_adul\"], 'sq_vals' : ['SQBdependency', 'SQBmeaned', 'SQBage', 'SQBhogar_nin', \n",
    "                                                                        'SQBhogar_total', 'SQBovercrowding', 'SQBedjefe', 'SQBescolari']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is a Naive Bayes Model?\n",
    "\n",
    "The Naive Bayes algorithm assumes that all features are independent of each other, meaning that the presence or absence of one feature does not affect the probability of another feature being present or absent. The algorithm uses Bayes' theorem to calculate the posterior probabilities of different classes given the observed evidence. Bayes' theorem allows us to update our beliefs about the probability of a hypothesis (such as the class of a data point) based on new evidence (such as the features of the data point).\n",
    "\n",
    "In this case, the Naive Bayes model calculates the probability of a household belonging to 1 of 4 target classes:\n",
    "\n",
    "- 1 = extreme poverty\n",
    "- 2 = moderate poverty\n",
    "- 3 = vulnerable households\n",
    "- 4 = non vulnerable households"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Summary of Approach\n",
    "\n",
    "**There are three main types of Naive Bayes models:**\n",
    "\n",
    "1. Gaussian Naive Bayes: Gaussian Naive Bayes assumes that the continuous data follows a normal distribution. In our dataset this includes things like age and years of education.\n",
    "\n",
    "2. Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age.\n",
    "\n",
    "3. Bernoulli Naive Bayes: Bernoulli Naive Bayes is also used for discrete data, the presence or absence of a certain attribute or feature. In our dataset this includes all the binary variables such as whether or not the floors are good and whether or not the dwelling has a toilet.\n",
    "\n",
    "*Each of these models can be run in isolation or in conjucntion with one another*\n",
    "\n",
    "There is also the Complement Naive Bayes Model which is a variation of the standard Naive Bayes algorithm developed to address the issue of imbalanced datasets. In an imbalanced dataset, the classes are not represented equally, meaning that there are more instances of one class than others. This can lead to poor classification performance, as the Naive Bayes algorithm may be biased towards the majority class.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Summary of Findings\n",
    "\n",
    "*We explored the following models. Accuracy refers to performance on trainng data.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Technique / Model                          | Accuracy |\n",
    "|--------------------------------------------|----------|\n",
    "| Standard Gaussian Naive Bayes              | 52%      |\n",
    "| Gaussian NB on Continuous Features ONLY    | 60%      |\n",
    "| Standard Multinomial Naive Bayes           | 48%      |\n",
    "| Multinomial NB on Count Features ONLY      | 53%      |\n",
    "| Standard Bernoulli Naive Bayes             | 60%      |\n",
    "| Bernoulli NB on Count Features ONLY        | 59%      |\n",
    "| MinMaxScalar on Bernoulli Naive Bayes      | 64%      |\n",
    "| MinMaxScalar on Multinomial Naive Bayes    | 19%      |\n",
    "| MinMaxScalar on Bernoulli Naive Bayes      | 59%      |\n",
    "| Model Ensembling (with MinMaxScalar)       | 59%      |\n",
    "| Complement Naive Bayes                     | 51%      |\n",
    "| Complement NB + MinMaxScaling              | 61%      |\n",
    "| Oversampling with SMOTE                    | 54%      |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models\n",
    "\n",
    "### 3.1 Basic Models\n",
    "\n",
    "#### Basic Gaussian Naive Bayes Model on Full Dataset\n",
    "Gaussian Naive Bayes: Gaussian Naive Bayes assumes that the continuous data follows a normal distribution. In our dataset this includes things like age and years of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.519327731092437\n",
      "F1 Score: 0.5416760750046212\n",
      "Recall: 0.519327731092437\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred) #evaluate_classification(y_pred, cm = True) to see Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes Model on Continuous Features ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.519327731092437\n",
      "F1 Score: 0.5416760750046212\n",
      "Recall: 0.519327731092437\n"
     ]
    }
   ],
   "source": [
    "X_g = X[nb_datatypes['gaussian']]\n",
    "X_valid_g = X_valid[nb_datatypes['gaussian']]\n",
    "gnb = GaussianNB()\n",
    "nb.fit(X_g, y)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Multinomial Naive Bayes on Full Dataset\n",
    "Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4756302521008403\n",
      "F1 Score: 0.4796126595434554\n",
      "Recall: 0.4756302521008403\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes on Count features ONLY\n",
    "Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6033613445378151\n",
      "F1 Score: 0.557918489887474\n",
      "Recall: 0.6033613445378151\n"
     ]
    }
   ],
   "source": [
    "X_m = X[nb_datatypes['multinomial']]\n",
    "X_valid_m = X_valid[nb_datatypes['multinomial']]\n",
    "gnb = MultinomialNB()\n",
    "nb.fit(X_m, y)\n",
    "y_pred = nb.predict(X_valid_m)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Bernoulli Naive Bayes on Full Dataset\n",
    "Bernoulli Naive Bayes: Bernoulli Naive Bayes is also used for discrete data, the presence or absence of a certain attribute or feature. In our dataset this includes all the binary variables such as whether or not the floors are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "F1 Score: 0.5975597697792903\n",
      "Recall: 0.6\n"
     ]
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes on binary features ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5949579831932773\n",
      "F1 Score: 0.5839627587591925\n",
      "Recall: 0.5949579831932773\n"
     ]
    }
   ],
   "source": [
    "X_b = X[nb_datatypes['bernoulli']]\n",
    "X_valid_b = X_valid[nb_datatypes['bernoulli']]\n",
    "bnb = BernoulliNB()\n",
    "nb.fit(X_b, y)\n",
    "y_pred = nb.predict(X_valid_b)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Techniques to improve our model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScalar\n",
    " MinMaxScaler is used to scale the numerical features of the dataset. Scaling is a common preprocessing step that can help improve the performance of certain machine learning algorithms, including the Multinomial Naive Bayes.\n",
    "\n",
    "- Feature scaling: MinMaxScaler scales the numerical features by subtracting the minimum value of the feature and dividing by the range (maximum value - minimum value) for each feature. This ensures that all the features have the same scale.\n",
    "\n",
    "- Handling non-negative data: MNB assumes that the input features follow a multinomial distribution, which requires non-negative values. By scaling the numerical features using MinMaxScaler, you ensure that all values are non-negative, satisfying the input requirements for MNB.\n",
    "\n",
    "- Equal weighting: When features have different scales, the algorithm might give more importance to features with larger values. Scaling the features ensures that they all have equal weight in the model, preventing any unintended biases based on the original scale of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Multinomial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6420168067226891\n",
      "F1 Score: 0.6193114634620565\n",
      "Recall: 0.6420168067226891\n"
     ]
    }
   ],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Gaussian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1915966386554622\n",
      "F1 Score: 0.16174923468243488\n",
      "Recall: 0.1915966386554622\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Bernoulli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5983193277310924\n",
      "F1 Score: 0.5959584104211549\n",
      "Recall: 0.5983193277310924\n"
     ]
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensembling (with MinMaxScalar)\n",
    "\n",
    "More than on Naive Bayes models can be combined when our features are of different data types. This technique is known as \"model stacking\" or \"ensembling.\" In this approach, we train separate Naive Bayes classifiers for different types of features and then combine their predictions to make a final decision. An ensemble model is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5932773109243697\n",
      "F1 Score: 0.5873286818040828\n",
      "Recall: 0.5932773109243697\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Gaussian features\n",
    "gaussian_columns = nb_datatypes['gaussian']\n",
    "\n",
    "# Bernoulli features\n",
    "bernoulli_columns = nb_datatypes['bernoulli']\n",
    "\n",
    "# Multinomial features\n",
    "multinomial_columns = nb_datatypes['multinomial']\n",
    "\n",
    "# Create Naive Bayes models\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit models on the respective features\n",
    "gnb.fit(X.iloc[:, :len(gaussian_columns)], y)\n",
    "bnb.fit(X.iloc[:, len(gaussian_columns):len(gaussian_columns) + len(bernoulli_columns)], y)\n",
    "mnb.fit(X.iloc[:, -len(multinomial_columns):], y)\n",
    "\n",
    "# Create an ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('gnb', gnb), ('bnb', bnb), ('mnb', mnb)], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble_model.fit(X, y)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = ensemble_model.predict(X_valid)\n",
    "evaluate_classification(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes \n",
    "\n",
    "Complement Naive Bayes (CNB) is an extension of Multinomial Naive Bayes that is particularly suited for imbalanced datasets. It is based on the assumption that the complement of a class has a similar distribution to that of the class itself. CNB uses a frequency-based approach, similar to MultinomialNB, but instead of counting the frequency of each feature in each class, it counts the frequency of each feature in the complement of each class.\n",
    "\n",
    "In order to use CNB with different data types, we can follow a similar approach as with MultinomialNB. First, we need to separate the data into different types, such as numerical, binary, and count data. We can then use a custom transformer to select the relevant features and transform them appropriately. For example, we can use a StandardScaler for numerical data, a BinaryEncoder for binary data, and a CountVectorizer for count data.\n",
    "\n",
    "Here's an example of how to use CNB with different data types in scikit-learn:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Complement Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5092436974789916\n",
      "F1 Score: 0.48583364033995385\n",
      "Recall: 0.5092436974789916\n"
     ]
    }
   ],
   "source": [
    "cnb = ComplementNB()\n",
    "cnb.fit(X, y)\n",
    "y_pred = cnb.predict(X_valid)\n",
    "evaluate_classification(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complement Naive Bayes + MinMaxScalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6050420168067226\n",
      "F1 Score: 0.5852309045186486\n",
      "Recall: 0.6050420168067226\n"
     ]
    }
   ],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "nb = ComplementNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Select categorical features\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "X_cat = X[cat_cols]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "enc = OneHotEncoder()\n",
    "X_cat_enc = enc.fit_transform(X_cat)\n",
    "\n",
    "# Select numerical features\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "X_num = X[num_cols]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "# Combine categorical and numerical features\n",
    "X_combined = np.hstack((X_cat_enc.toarray(), X_num_scaled))\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=10)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model on a validation set\n",
    "X_valid_combined = X_valid  # Use the already combined validation data\n",
    "y_pred = nb.predict(X_valid_combined)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limitations and Next Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Naive Bayes Model performed with between 50% and 60% accuracy on training data across various iterations of the model. The MinMaxScalar seemed to be an important step to improve accruacy and the Bernoulli Naive Bayes with MinMaxScalar performed the best of all models tested, with 64% accuracy.\n",
    "\n",
    "##### The benefits of a Naive Bayes model for this project are:\n",
    "- **Scalability:** Naive bays can handle large datasets and high-dimensional feature spaces efficiently, as the computational complexity of training is linear with the number of features. We have a lot of features in our dataset, and this model is able to parse through many features quickly\n",
    "- **Speed:** Naive Bayes models are fast to train and predict. This allows us to run many different versions and experiment with different structures and parameters.\n",
    "\n",
    "##### The drawbacks of a Naive Bayes model for this project are:\n",
    "- **Independence assumption:** Naive Bayes models assume that features are conditionally independent given the class label. *This is almost certainly not true with this data.*\n",
    "\n",
    "- **Data scarcity:** The performance of Naive Bayes models can suffer when there is not enough data to estimate the probabilities accurately. *We know we have a data scarcity problem with this dataset*\n",
    "\n",
    "- **Continuous features:** Naive Bayes models work better with categorical data. *We have a lot of bianry and continuous data*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capp-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
