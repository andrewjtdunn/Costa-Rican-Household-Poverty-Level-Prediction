{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Andrew Dunn, Katherine Dumais, Kathryn Link-Oberstar, Lee-Or Bentovim\n",
            "\n",
            "#### Summary of initial feature engineering and model testing for checkpoint 2\n",
            "\n",
            "This analysis runs after we load, clean, and generate additional variables in load_data.py, and initially explored the data in exploratory_analysis.ipynb. This document summarizes the best performing models of the different types that we evaluated. A complete documentation of our research into different models can be found in these notebooks:\n",
            "- [Logistic Regression](https://github.com/andrewjtdunn/Costa-Rican-Household-Poverty-Level-Prediction/blob/main/Logistic%20Regression-final.ipynb)\n",
            "- [K-Nearest Neigbors](https://github.com/andrewjtdunn/Costa-Rican-Household-Poverty-Level-Prediction/blob/main/knn.ipynb)\n",
            "- [Naive Bayes](https://github.com/andrewjtdunn/Costa-Rican-Household-Poverty-Level-Prediction/blob/main/Naive_Bayes.ipynb)\n",
            "- [Random Forest](https://github.com/andrewjtdunn/Costa-Rican-Household-Poverty-Level-Prediction/blob/main/Random_Forest.ipynb)\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Outline of Notebook\n",
            "**[1. Project Set-Up](#project-set-up)** \\\n",
            "*[1.1 Load Packages and Data](#1.1-load-packages-and-data)* \\\n",
            "*[1.2 Feature Engineering](#1.2-feature-engineering)* \\\n",
            "\\\n",
            "**[2. Model Testing](#2.-model-testing)** \\\n",
            "*[2.1 Random Forest](#2.1-random-forest)* \\\n",
            "*[2.2 Naive Bayes](#2.2-naive-bayes)* \\\n",
            "*[2.3 K Nearest Neighbors](#2.3-k-nearest-neighbors)* \\\n",
            "*[2.4 Logistic Regression](#2.4-logistic-regression)* \\\n",
            "\\\n",
            "**[3. Discussion](#3-discussion)** \\\n",
            "\\\n",
            "**[4. Next Steps](#4-next-steps)** "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 1. Project Set-Up"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 1.1 Load Packages and Data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "import numpy as np\n",
            "import os\n",
            "import matplotlib.pyplot as plt\n",
            "from imblearn.over_sampling import RandomOverSampler, SVMSMOTE\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import RandomizedSearchCV\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from collections import Counter\n",
            "from sklearn.feature_selection import RFE\n",
            "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
            "from sklearn.preprocessing import MinMaxScaler\n",
            "from sklearn.model_selection import train_test_split\n",
            "SEED = 12\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Solving ipynb hating relative imports\n",
            "current_dir = os.getcwd()\n",
            "parent_dir = os.path.dirname(current_dir)\n",
            "os.chdir(parent_dir)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import load_data\n",
            "from evaluate_classification import evaluate_classification\n",
            "import analyze_k"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df, indices = load_data.load_train_data()\n",
            "X = df.iloc[:, :-1]\n",
            "y = df.loc[:, 'Target']\n",
            "\n",
            "# Creates dictionaries with each K-fold, where train_indices has the indices to include and test_indices is the validation set\n",
            "train_indices = {}\n",
            "test_indices = {}\n",
            "\n",
            "for i, (train_index, test_index) in enumerate(indices):\n",
            "    train_indices[i] = train_index\n",
            "    test_indices[i] = test_index"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 1.2 Feature Engineering\n",
            "\n",
            "We generate several new variables, derived from our literature review, in load_data.py:\n",
            "\n",
            "- max_education_level: the education level of the person with the highest education level in the household\n",
            "- hh_has_marriage: whether someone in the household is married\n",
            "- hh_max_age: the age of the oldest person in the household\n",
            "- hh_sex_ratio: the ratio of men to women in the household\n",
            "- hh_child_woman_ratio_12: the ratio of children to women in the household, with children defined as being under 12\n",
            "- hh_child_adult_ratio_12: the ratio of children to adults in the household, with children defined as being under 12\n",
            "- hh_child_woman_ratio_19: the ratio of children to women in the household, with children defined as being under 19\n",
            "- hh_child_adult_ratio_19: the ratio of children to adults in the household, with children defined as being under 19\n",
            "- v2a1_log: the logged value of v2a1, the monthly rent payment. We also estimate and impute values into v2a1 when that field is missing."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2. Model Testing\n",
            "\n",
            "We tested variations of four different models: random forest, naive bayes, KNN, and logistic regression in the following notebooks:\n",
            "\n",
            "Below, we demonstrate the best performing variants of those models."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 2.1 Random Forest\n",
            "\n",
            "A Random Forest model is a machine learning algorithm that creates many decision trees and averages the results from them. Because of this, random forests have the benefit over decision trees of being less likely to overfit on the training data. They are frequently used for classification questions. \n",
            "\n",
            "We run random search cross validation on three different sets of data: \n",
            "- the raw cleaned data\n",
            "- data where we randomly oversample from the underrepresented classes\n",
            "- data where we apply the SMOTE methodology to generate additional rows for the underrepresented classes\n",
            "\n",
            "The random search cross validation process randomly selects different combinations of hyperparameters and returns the best fitting.\n",
            "\n",
            "Thus far, there is no significant difference between the models run on the different datasets. The best performing models from the random search cross validation process yields an accuracy of about .65 and an f1 score of about .59 when run on the validation data."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = RandomForestClassifier(random_state = SEED,\n",
            "                            n_estimators = 1600,\n",
            "                            min_samples_split = 2,\n",
            "                            min_samples_leaf = 1,\n",
            "                            max_features = 'sqrt',\n",
            "                            max_depth = 100,\n",
            "                            bootstrap = False\n",
            ")\n",
            "clf_results = {}\n",
            "for key in train_indices.keys():\n",
            "    X_train = df.drop(columns=\"Target\").iloc[train_indices[key],:]\n",
            "    y_train = df.loc[:,['Target']].iloc[train_indices[key],:]\n",
            "    X_valid = df.drop(columns=\"Target\").iloc[test_indices[key],:]\n",
            "    y_valid = df.loc[:,['Target']].iloc[test_indices[key],:]\n",
            "    clf.fit(X_train, y_train)\n",
            "    y_pred = clf.predict(X_valid)\n",
            "    clf_results[key] = evaluate_classification(y_pred, y_valid, cm = True, return_vals=True)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 2.2 Naive Bayes\n",
            "\n",
            "The Naive Bayes algorithm assumes that all features are independent of each other, meaning that the presence or absence of one feature does not affect the probability of another feature being present or absent. The algorithm uses Bayes' theorem to calculate the probabilities of different classes given the observed evidence. \n",
            "\n",
            "The Naive Bayes model is Scalable, can handle large datasets and high-dimensional feature spaces efficiently. However, one of the key assumptions of Naive Bayes is that features are conditionally independent given the class label. *This is almost certainly not true with this data.* \n",
            "Some other imitation of Naive Bayes include:\n",
            "* *Data Scarcity*: Naive Bayes models can suffer when there is not enough data to estimate the probabilities accurately\n",
            "* *Continuous features:* Naive Bayes models work better with categorical data. *We have a lot of bianry and continuous data*\n",
            "* *Imbalanced Datasets*: Naive Bayes models may not perform well on an imbalanced dataset, where the classes are not represented equally. This can lead to poor classification performance, as the Naive Bayes algorithm may be biased towards the majority class."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**MinMaxScalar on Multinomial Naive Bayes**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "scaler = MinMaxScaler()\n",
            "nb = MultinomialNB()\n",
            "nb_results = {}\n",
            "for key in train_indices.keys():\n",
            "    X_train = df.drop(columns=\"Target\").iloc[train_indices[key],:]\n",
            "    y_train = df.loc[:,['Target']].iloc[train_indices[key],:]\n",
            "    X_valid = df.drop(columns=\"Target\").iloc[test_indices[key],:]\n",
            "    y_valid = df.loc[:,['Target']].iloc[test_indices[key],:]\n",
            "    X_scaled = scaler.fit_transform(X_train)\n",
            "    nb.fit(X_scaled, y_train)\n",
            "    X_valid_scaled = scaler.transform(X_valid)\n",
            "    y_pred = nb.predict(X_valid_scaled)\n",
            "    nb_results[key] = evaluate_classification(y_pred, y_valid, cm = True, return_vals=True)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Complement Naive Bayes + MinMaxScalar**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "nb = ComplementNB()\n",
            "nb_results = {}\n",
            "for key in train_indices.keys():\n",
            "    X_train = df.drop(columns=\"Target\").iloc[train_indices[key],:]\n",
            "    y_train = df.loc[:,['Target']].iloc[train_indices[key],:]\n",
            "    X_valid = df.drop(columns=\"Target\").iloc[test_indices[key],:]\n",
            "    y_valid = df.loc[:,['Target']].iloc[test_indices[key],:]\n",
            "    X_scaled = scaler.fit_transform(X_train)\n",
            "    nb.fit(X_scaled, y_train)\n",
            "    X_valid_scaled = scaler.transform(X_valid)\n",
            "    y_pred = nb.predict(X_valid_scaled)\n",
            "    nb_results[key] = evaluate_classification(y_pred, y_valid, cm = True, return_vals=True)\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "In general, the Naive Bayes Model performed with between 50% and 60% accuracy on training data across various iterations of the model.\n",
            "\n",
            "Main Takeaways: \n",
            "* The model with the best accuracy was the **Bernoulli Naive Bayes with MinMaxScalar**. It had 65% accuracy, and even this model is one of the best performing, its accuracy is primarily due to the fact that it classfies 4, our over represented class, well. The recall values for this model are. Label 1: 0.29 Label 2: 0.4 Label 3: 0.14 Label 4: 0.86\n",
            "* One of the biggest challenged we face in this project is figuring out how to handle the overrepresenation of group 4 in our data. Models that classfiy most data as 4 (the over represented class) may return higher accuracy but may be overall less useful. \n",
            "    * In general, models seemed to behave in 1 of 2 ways:\n",
            "        1. Classifying most values as 4 (i.e. the *Bernoulli Naive Bayes with MinMaxScalar* above), or\n",
            "        2. Split values between 2 and 4 (i.e. *Complement Naive Bayes + MinMaxScalar*)\n",
            "    * **Complement Naive Bayes + MinMaxScalar**: This model has some of the highest accuracy of all the Naive Bayes Models (61%) and is also the best model classfiying 2: Recall: Label 1: 0.0 Label 2: 0.57 Label 3: 0.1 Label 4: 0.79\n",
            "* Overall, MinMaxScalar seemed to be an important step to improve accuracy across the board."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 2.3 K Nearest Neighbors\n",
            "\n",
            "For the KNN section, we attempted to build K Nearest Neighbors models with a variety of different inputs using SKlearn's KNN classifiers. We considered the following approach: using uniform or distance based weighting, changing the number of nearest features to select, regularizing the data, and considering a radius based as opposed to nearest neighbors based approach. In general, the results were disappointing, see below table for the full outcomes. I've reproduced the highest performing model below as an example of the model that performed best, however it is useful to note that its recall score shows that even this model is only really correctly classifying data in class 4."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "neigh = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
            "knn_results = {}\n",
            "for key in train_indices.keys():\n",
            "    X_train = df.drop(columns=\"Target\").iloc[train_indices[key],:]\n",
            "    y_train = df.loc[:,['Target']].iloc[train_indices[key],:]\n",
            "    X_valid = df.drop(columns=\"Target\").iloc[test_indices[key],:]\n",
            "    y_valid = df.loc[:,['Target']].iloc[test_indices[key],:]\n",
            "    neigh.fit(X_train,y_train)\n",
            "    pred = neigh.predict(X_valid)\n",
            "    knn_results[key] = evaluate_classification(pred,y_valid,cm=True, return_vals=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Here's how to get average outcome\n",
            "# average_outcome = analyze_k.average_outcome(knn_results)\n",
            "\n",
            "# Here's an example of how to get the index (which K pass) has the best recall score\n",
            "# This example is for class 4, but works for all classes.\n",
            "# If accuracy/f1, don't include a class number\n",
            "# analyze_k.select_best(knn_results,'recall',4)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "\\begin{array}{c|ccc}\n",
            "\\text{} & \\text{accuracy} & \\text{f1} & \\text{recall [1,2,3,4]} \\\\\n",
            "\\hline\n",
            "k\\_15\\_distance   &      0.60 &  0.53 &     [0.04, 0.1, 0.1, 0.9] \\\\\n",
            "k\\_20\\_distance   &      0.61 &  0.53 &  [0.04, 0.09, 0.12, 0.91] \\\\\n",
            "k\\_3\\_uniform     &      0.53 &  0.52 &  [0.22, 0.23, 0.07, 0.74] \\\\\n",
            "k\\_5\\_uniform     &      0.55 &  0.52 &  [0.06, 0.19, 0.11, 0.79] \\\\\n",
            "k\\_10\\_uniform    &      0.60 &  0.52 &  [0.02, 0.11, 0.04, 0.91] \\\\\n",
            "k\\_10\\_distance   &      0.58 &  0.52 &   [0.04, 0.1, 0.12, 0.86] \\\\\n",
            "k\\_15\\_uniform    &      0.62 &  0.52 &  [0.06, 0.07, 0.04, 0.94] \\\\\n",
            "k\\_20\\_uniform    &      0.62 &  0.52 &  [0.02, 0.06, 0.04, 0.96] \\\\\n",
            "k\\_3\\_distance    &      0.52 &  0.51 &  [0.08, 0.14, 0.18, 0.75] \\\\\n",
            "k\\_5\\_distance    &      0.54 &  0.51 &  [0.06, 0.12, 0.14, 0.79] \\\\\n",
            "re\\_20\\_distance  &      0.46 &  0.50 &  [0.29, 0.33, 0.34, 0.53] \\\\\n",
            "rad\\_3\\_distance  &      0.63 &  0.49 &      [0.0, 0.0, 0.0, 1.0] \\\\\n",
            "rad\\_5\\_distance  &      0.63 &  0.49 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
            "re\\_15\\_distance  &      0.45 &  0.49 &  [0.31, 0.32, 0.32, 0.53] \\\\\n",
            "rad\\_3\\_uniform   &      0.63 &  0.49 &      [0.0, 0.0, 0.0, 1.0] \\\\\n",
            "re\\_10\\_distance  &      0.45 &  0.49 &  [0.37, 0.28, 0.33, 0.52] \\\\\n",
            "re\\_5\\_distance   &      0.45 &  0.49 &   [0.24, 0.25, 0.3, 0.56] \\\\\n",
            "rad\\_20\\_uniform  &      0.62 &  0.49 &    [0.0, 0.0, 0.01, 0.98] \\\\\n",
            "rad\\_20\\_distance &      0.62 &  0.49 &    [0.0, 0.0, 0.01, 0.98] \\\\\n",
            "rad\\_5\\_uniform   &      0.63 &  0.49 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
            "re\\_3\\_distance   &      0.44 &  0.48 &  [0.24, 0.24, 0.27, 0.55] \\\\\n",
            "rad\\_10\\_distance &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
            "rad\\_15\\_distance &      0.62 &  0.48 &     [0.0, 0.0, 0.0, 0.98] \\\\\n",
            "rad\\_15\\_uniform  &      0.62 &  0.48 &     [0.0, 0.0, 0.0, 0.98] \\\\\n",
            "rad\\_10\\_uniform  &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
            "re\\_3\\_uniform    &      0.43 &  0.47 &  [0.37, 0.32, 0.16, 0.51] \\\\\n",
            "re\\_5\\_uniform    &      0.41 &  0.46 &  [0.35, 0.34, 0.25, 0.47] \\\\\n",
            "re\\_15\\_uniform   &      0.40 &  0.45 &  [0.35, 0.34, 0.26, 0.45] \\\\\n",
            "re\\_20\\_uniform   &      0.40 &  0.45 &  [0.39, 0.36, 0.27, 0.43] \\\\\n",
            "re\\_10\\_uniform   &      0.39 &  0.44 &  [0.43, 0.31, 0.29, 0.42] \\\\\n",
            "\\end{array}"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The KNN approach suffered from some issues we were not able to address, leading us to conclude this was not the right model. KNN uses all features by default, but our attempts at selecting only certain features was if anything less successful than using all features. Likewise, an attempt to break out 4 from the rest and then classify the rest between 1,2,3 did worse than a one stage classifier. Finally, when we tried to oversample our classes 1-3, the KNN model continued to predict only class 4. While we will continue to use some of these approaches to better classify our data, it is clear to us that KNN does not provide the best path forwards."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 2.4 Logistic Regression\n",
            "\n",
            "The goal of logistic regression is to find the best parameters of a logistic function that minimizes the difference between the predicted probabilities and the actual outcomes. The logistic function uses the sigmoid function, which maps any real number into a range between 0 and 1, allowing us to interpret the output as a probability. The input features are weighted and combined linearly, and the resulting value is passed through the logistic function to produce a probability. We then use gradient descent to determine what the best classification is for these categories. Regularization helps to improve the generalization performance of the model by balancing the bias-variance trade-off and reducing overfitting.  By penalizing the weights of the input features, regularization encourages the model to focus on the most important features that are most relevant to the target variable. Overfitting occurs when a model learns to fit the training data too closely, including noise and irrelevant features, leading to poor generalization performance on new, unseen data. In this case we will try L2 regularization which adds a penalty term proportional to the square of the weights.\n",
            "\n",
            "One drawback however, is that it assumes linear relationships which based on the diversity of our data, may not be true in this case. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ros = RandomOverSampler(random_state=67)\n",
            "\n",
            "#Train our data with two oversampling methods: SVM SMOTE and random sampling with replacement \n",
            "X_train, X_valid, y_train, y_valid = train_test_split(\n",
            "        df.drop(columns=\"Target\"),\n",
            "        df.loc[:, [\"Target\"]],\n",
            "        test_size=0.2,\n",
            "        random_state = 42,\n",
            "    )\n",
            "train_df_resampled, train_y_resampled = ros.fit_resample(X_train, y_train)\n",
            "\n",
            "sm = SVMSMOTE(random_state=67)\n",
            "X_smote, y_smote = sm.fit_resample(X_train, y_train)\n",
            "print(len(X_smote))\n",
            "\n",
            "#Train our data with two oversampling methods: SVM SMOTE and random sampling with replacement \n",
            "print(\"With Smote- RFE\")\n",
            "#Recursive feature elimination\n",
            "rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 6, step = 1)\n",
            "rfe.fit(X_smote, y_smote)\n",
            "y_pred = rfe.predict(X_valid)\n",
            "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n",
            "\n",
            "print(\"Features selected:With Smote- RFE\")\n",
            "print(X_train.iloc[:,[64,138,126,127,107,108,6]].columns)\n",
            "\n",
            "print(\"With Random Selection- RFE\")\n",
            "#Recursive feature elimination\n",
            "rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 6, step = 1)\n",
            "rfe.fit(train_df_resampled, train_y_resampled)\n",
            "y_pred = rfe.predict(X_valid)\n",
            "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n",
            "print(\"Features selected:With Random Selection- RFE\")\n",
            "print(X_train.iloc[:,[36,37,42,45,107,108]].columns)\n",
            "\n",
            "\n",
            "print(\"With Smote- Variance Threshold\")\n",
            "# get rid of features without a lot of variance \n",
            "from sklearn.feature_selection import VarianceThreshold\n",
            "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
            "a= X_smote.copy()\n",
            "rev_x = sel.fit_transform(a)\n",
            "\n",
            "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, y_smote)\n",
            "y_pred = reg.predict(sel.transform(X_valid.copy()))\n",
            "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n",
            "\n",
            "print(\"With Random Selection-Variance threshold\")\n",
            "\n",
            "# get rid of features without a lot of variance \n",
            "from sklearn.feature_selection import VarianceThreshold\n",
            "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
            "rev_x = sel.fit_transform(train_df_resampled.copy())\n",
            "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, train_y_resampled)\n",
            "a= X_valid.copy()\n",
            "y_pred = reg.predict(sel.transform(a))\n",
            "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 6, step = 1)\n",
            "sm = SVMSMOTE(random_state=42)\n",
            "for key in train_indices.keys():\n",
            "    X_train = df.drop(columns=\"Target\").iloc[train_indices[key],:]\n",
            "    y_train = df.loc[:,['Target']].iloc[train_indices[key],:]\n",
            "    X_valid = df.drop(columns=\"Target\").iloc[test_indices[key],:]\n",
            "    y_valid = df.loc[:,['Target']].iloc[test_indices[key],:]\n",
            "    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
            "    rev_x = sel.fit_transform(X_train.copy())\n",
            "    reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, y_train)\n",
            "    a= X_valid.copy()\n",
            "    y_pred = reg.predict(sel.transform(a))\n",
            "    evaluate_classification(y_pred, y_valid, [1,2,3,4], cm = True)\n",
            "    \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import warnings\n",
            "warnings.filterwarnings('ignore')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "While, if we reference the logistic regression function, without overweighting we only classify the majority class 4 (please see regression document). With overweighting and feature selection methods, we get our best predictors using regularized logistic regression in terms of classification accuracy and F1 score. But given the diversity of the results, it is clear feature selection as opposed to logistic regression usage is the most important part of this work. It is unclear if regularized logistic regression will be the best indicator going forward."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 3. Discussion\n",
            "\n",
            "- Logistic regression and random forest models performed the best of the model types we evaluated. Our best performing model was logistic regression with SVM-SMOTE oversampling which yielded an accuracy of .67 and f1 score of .66. Random forests and naive bayes also performed relatively well, but had slightly lower scores.\n",
            "\n",
            "- Most of the models tested performed well at identifying label 4, but performed significantly worse at identifying the other labels. For example, none of the KNN models had a recall score higher than .43 for a label other than 4, and the best performing Random Forest model had a highest recall score of .3 for labels other than 4. Some Naive Bayes models had a recall score of .57 for label 2. This model may be useful as we turn to ensemble modeling.\n",
            "\n",
            "\n",
            "\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 4. Next Steps\n",
            "\n",
            "Experiment with Different Preprocessing Strategies:\n",
            "- Feature Engineering\n",
            "     - Experiment with the weighting of different features\n",
            "     - Min/max scaling appears to have been valuable in our Naive Nayes models. Implementing this data preprocessing step for the other models may improve their performance.\n",
            "     - In this same vein, variance thresholds with overfitted models had some success in predicting diverse categories, as opposed to just 4. \n",
            "- Experiment with different ways to address class imbalance:\n",
            "     - Split the data into binary targets (4 or not-4) to better categorize the non-4 labels. (i.e. multiclass.OneVsRestClassifier)\n",
            "     - Experiment further with a weighting of different classes\n",
            "     - Some of our existing models that did show a propensity for classifying classes other than 4 may be useful here as well in concert with other models and techniques\n",
            "\n",
            "Model Ensembling:\n",
            "- Determine how to run all models multiple times and average the results to ensure outliers do not unduly influence our results.\n",
            "- Implement various ensemble modeling strategies including but not limited to:\n",
            "ADA boost (initial experimentation appears to improve model performance)\n",
            "Bagging.\n",
            "\n",
            "With the above techniques, we hope to gain further accuracy at predicting the individual categories over the majority category, and improve our overall F1 scores. \n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "costa-rican-household-poverty-level-predic-mFobo9k6-py3.11",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.2"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
