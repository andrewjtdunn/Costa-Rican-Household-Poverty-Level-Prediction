{"cells":[{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.023810Z","iopub.status.busy":"2023-05-22T06:46:13.023425Z","iopub.status.idle":"2023-05-22T06:46:13.031789Z","shell.execute_reply":"2023-05-22T06:46:13.030460Z","shell.execute_reply.started":"2023-05-22T06:46:13.023781Z"},"trusted":true},"outputs":[],"source":["# Load and clean data\n","import pandas as pd\n","import numpy as np\n","import json\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_selection import VarianceThreshold\n","import warnings\n","from imblearn.over_sampling import RandomOverSampler, SVMSMOTE\n","import os\n","\n","SEED = 2023\n","\n","# for kaggle\n","train_data = \"/kaggle/input/costa-rican-household-poverty-prediction/train.csv\"\n","test_data = \"/kaggle/input/costa-rican-household-poverty-prediction/test.csv\"\n","var_descriptions = \"/kaggle/input/var-descriptions/var_descriptions.json\"\n","\n","# locally\n","train_data = \"Kaggle_download/train.csv\"\n","test_data = \"Kaggle_download/test.csv\"\n","var_descriptions = \"var_descriptions.json\""]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.037362Z","iopub.status.busy":"2023-05-22T06:46:13.036929Z","iopub.status.idle":"2023-05-22T06:46:13.067876Z","shell.execute_reply":"2023-05-22T06:46:13.066775Z","shell.execute_reply.started":"2023-05-22T06:46:13.037328Z"},"trusted":true},"outputs":[],"source":["def load_train_data(filepath=train_data, seed=SEED):\n","    \"\"\"\n","    Loads, cleans, and imputes new variables in Kaggle data\n","\n","    Input: \n","        file (csv): optional, default is the train data\n","        seed (int): optional seed\n","\n","    Returns:\n","        df (dataframe), composed of X_train and y_train\n","        X_valid (dataframe)\n","        y_valid (dataframe)\n","    \"\"\"\n","    # Load data\n","    df = pd.read_csv(filepath)\n","\n","    # Clean a couple data fields\n","    ###########################################################################\n","    # see here for info: https://www.kaggle.com/competitions/costa-rican-household-poverty-prediction/discussion/61751\n","\n","    # edjefe\n","    df.loc[df.loc[:, \"edjefe\"] == \"yes\", \"edjefe\"] = 1\n","    df.loc[df.loc[:, \"edjefe\"] == \"no\", \"edjefe\"] = 0\n","    df[\"edjefe\"] = df[\"edjefe\"].astype(str).astype(int)\n","\n","    # edjefa\n","    df.loc[df.loc[:, \"edjefa\"] == \"yes\", \"edjefa\"] = 1\n","    df.loc[df.loc[:, \"edjefa\"] == \"no\", \"edjefa\"] = 0\n","    df[\"edjefa\"] = df[\"edjefa\"].astype(str).astype(int)\n","\n","    # ASSUME DEPENDENCY HAS THE SAME MISCODING\n","    # https://www.kaggle.com/competitions/costa-rican-household-povertyx-prediction/discussion/73055\n","    df.loc[df.loc[:, \"dependency\"] == \"yes\", \"dependency\"] = 1\n","    df.loc[df.loc[:, \"dependency\"] == \"no\", \"dependency\"] = 0\n","    df[\"dependency\"] = df[\"dependency\"].astype(str).astype(float)\n","\n","    # Fix NAs for number of tablets owned\n","    df.loc[:, \"v18q1\"] = df.loc[:, \"v18q1\"].fillna(0)\n","\n","    # Create new individual-level variables base on lit review\n","    ###########################################################################\n","\n","    # highest level of education in household\n","    def get_max_education_level(row):\n","        education_levels = [\n","            row[\"instlevel1\"],\n","            row[\"instlevel2\"],\n","            row[\"instlevel3\"],\n","            row[\"instlevel4\"],\n","            row[\"instlevel5\"],\n","            row[\"instlevel6\"],\n","            row[\"instlevel7\"],\n","            row[\"instlevel8\"],\n","            row[\"instlevel9\"],\n","        ]\n","        return max(education_levels)\n","\n","    # Create a new column in the DataFrame representing the highest education level in a household\n","    df[\"max_education_level\"] = df.apply(get_max_education_level, axis=1)\n","\n","    # if there is a marriage in the household\n","    df.loc[:, \"hh_has_marriage\"] = (\n","        df.loc[:, \"estadocivil3\"].groupby(df.loc[:, \"idhogar\"]).transform(\"max\")\n","    )\n","\n","    # max age in household\n","    df.loc[:, \"hh_max_age\"] = (\n","        df.loc[:, \"age\"].groupby(df.loc[:, \"idhogar\"]).transform(\"max\")\n","    )\n","\n","    # sex ratio in household\n","    # #male/#female\n","    df.loc[:, \"hh_sex_ratio\"] = df.loc[:, \"r4h3\"] / df.loc[:, \"r4m3\"]\n","\n","    # child/woman ratio in household\n","    # children defined as under 12\n","    # women defined as 12 and over\n","    df.loc[:, \"hh_child_woman_ratio_12\"] = df.loc[:, \"r4t1\"] / df.loc[:, \"r4m3\"]\n","\n","    # child/adult ratio in household\n","    # children defined as under 12\n","    # adults defined as 12 and over\n","    df.loc[:, \"hh_child_adult_ratio_12\"] = df.loc[:, \"r4t1\"] / df.loc[:, \"r4t2\"]\n","\n","    # child/woman ratio in household\n","    # children defined as under 19\n","    # women defined as 12 and over\n","    # THIS IS A DATA QUALITY ISSUE -- CATS AREN'T MUTUALLY EXCLUSIVE\n","    df.loc[:, \"hh_child_woman_ratio_19\"] = df.loc[:, \"hogar_nin\"] / df.loc[:, \"r4m2\"]\n","\n","    # child/adult ratio in household\n","    # children defined as under 19\n","    # adults defined as 19 and over\n","    df.loc[:, \"hh_child_adult_ratio_19\"] = df.loc[:, \"hogar_nin\"] / df.loc[:, \"hogar_adul\"]\n","\n","    # Reshape the data to be at household level rather than individual level\n","    ###########################################################################\n","\n","    # pick the head of the household\n","    df.loc[df.loc[:, \"parentesco1\"] == 1, \"hh_head\"] = 1\n","\n","    # create temp vars to determine if household head exists and max age in household\n","    df.loc[:, \"hh_head_exists\"] = df.groupby([df.loc[:, \"idhogar\"]])[\n","        \"hh_head\"\n","    ].transform(max)\n","\n","    # in instances where there isn't a head of household, pick the oldest male\n","    df.loc[\n","        (\n","            (df.loc[:, \"hh_head_exists\"] == 0)\n","            & (df.loc[:, \"age\"] == df.loc[:, \"hh_max_age\"])\n","            & (df.loc[:, \"male\"] == 1)\n","        ),\n","        \"hh_head\",\n","    ] = 1\n","\n","    # update the temp hh head flag var\n","    df.loc[:, \"hh_head_exists\"] = df.groupby([df.loc[:, \"idhogar\"]])[\n","        \"hh_head\"\n","    ].transform(max)\n","\n","    # in instances where there isn't an oldest male, pick the oldest\n","    df.loc[\n","        (\n","            (df.loc[:, \"hh_head_exists\"] == 0)\n","            & (df.loc[:, \"age\"] == df.loc[:, \"hh_max_age\"])\n","        ),\n","        \"hh_head\",\n","    ] = 1\n","\n","    # collapse the data\n","    df = df.loc[df.loc[:, \"hh_head\"] == 1]\n","\n","    # drop the temp var and other household head vars\n","    df = df.drop(columns=[\"hh_head_exists\", \"parentesco1\", \"hh_head\"])\n","\n","    # Create household-level variables\n","    ###########################################################################\n","\n","    with open(var_descriptions, \"r\") as f:\n","        # Load JSON data as a dictionary\n","        var_desc = json.load(f)\n","\n","    features_to_include = [x for x in var_desc.keys() if x not in [\n","            \"Id\",\n","            \"idhogar\",\n","            \"dependency\",\n","            \"rez_esc\",\n","            \"hh_head\",\n","            \"parentesco1\",\n","            \"hh_head_exists\",\n","            \"Target\"\n","        ]\n","    ]\n","    df_subset = df[features_to_include]\n","\n","    # impute mean rent values while suppressing error message\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(action='ignore', category=Warning)\n","        imp_mean = IterativeImputer(random_state=0, n_nearest_features=5)\n","        imp_mean.fit(df_subset)\n","        mean_subset = imp_mean.transform(df_subset)\n","\n","    # replace 0s    \n","    df.loc[:, \"v2a1\"] = mean_subset[:, 0]\n","    df.loc[df.loc[:, \"v2a1\"] < 0, \"v2a1\"] = 0\n","\n","    # define logged value of v2a1, it provides a better distribution\n","    df[\"v2a1_log\"] = np.log1p(df[\"v2a1\"])\n","\n","    # Clean up NAs and inf values\n","    cols_to_drop = ['Id', 'idhogar', 'rez_esc']\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    for col in cols_to_drop:\n","        if col in df.columns:\n","            df.drop(col, axis=1, inplace=True)\n","    df.fillna(df.mean(), inplace=True)\n","\n","    #train_indices, valid_indices = implement_kfold(df)\n","\n","    return df\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.089488Z","iopub.status.busy":"2023-05-22T06:46:13.088752Z","iopub.status.idle":"2023-05-22T06:46:13.098462Z","shell.execute_reply":"2023-05-22T06:46:13.097050Z","shell.execute_reply.started":"2023-05-22T06:46:13.089443Z"},"trusted":true},"outputs":[],"source":["def implement_kfold(df, n_splits=5, shuffle=True, random_state=SEED):\n","    \"\"\"\n","    This helper function implements stratified k-fold cross validation. \n","        Primarily called within the load_data function but can be called \n","        independently\n","\n","    Inputs:\n","    df (DataFrame): a dataframe with features and a target column\n","    n_splits (int, optional): k, the number of splits to make in the dataframe\n","    shuffle (Bool, optional): Whether to shuffle each class's samples before \n","        splitting into batches.\n","    random_state (int, optional): the random seed to set for replicability\n","\n","    Outputs:\n","    train_indices (dictionary): A dictionary with keys referring to an \n","        individual fold (from k-fold) and values referring to the indexes to \n","        include as training data for that pass\n","    valid_indices (dictionary): A dictionary with keys referring to an \n","        individual fold (from k-fold) and values referring to the indexes to \n","        include as validation data for that pass\n","    \"\"\"\n","\n","    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n","    indices = skf.split(df.drop(columns=\"Target\"), df.loc[:, [\"Target\"]])\n","\n","    train_indices = {}\n","    valid_indices = {}\n","\n","    for i, (train_index, valid_index) in enumerate(indices):\n","        train_indices[i] = train_index\n","        valid_indices[i] = valid_index\n","\n","    return train_indices, valid_indices"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.101197Z","iopub.status.busy":"2023-05-22T06:46:13.100756Z","iopub.status.idle":"2023-05-22T06:46:13.114691Z","shell.execute_reply":"2023-05-22T06:46:13.113586Z","shell.execute_reply.started":"2023-05-22T06:46:13.101159Z"},"trusted":true},"outputs":[],"source":["def gen_SMOTE_data(df, seed = SEED):\n","    '''\n","    Generate SMOTE dataframes.\n","\n","    Inputs:\n","        df (dataframe): data and labels \n","        seed (int): optional seed\n","    \n","    Returns:\n","        X_smote (dataframe): the resampled data\n","        y_smote (dataframe): the resampled labels\n","    '''\n","    X = df.drop(columns='Target')\n","    y = df.loc[:, 'Target']\n","\n","    sm = SVMSMOTE(random_state = seed)\n","    X_smote, y_smote = sm.fit_resample(X, y)\n","\n","    return X_smote, y_smote"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.116635Z","iopub.status.busy":"2023-05-22T06:46:13.115934Z","iopub.status.idle":"2023-05-22T06:46:13.126402Z","shell.execute_reply":"2023-05-22T06:46:13.125499Z","shell.execute_reply.started":"2023-05-22T06:46:13.116601Z"},"trusted":true},"outputs":[],"source":["def best_model(df):\n","    \"\"\"\n","    Defines the best model and trains it on all training data\n","\n","    Input (dataframe): the cleaned training data\n","\n","    Returns:\n","        log_reg (model object): the trained model\n","        col_names (list): the names of the most informative columns, as chosen\n","            by the model\n","    \"\"\"\n","    sel = VarianceThreshold(threshold=0.16)\n","    X = df.drop(columns=\"Target\").copy()\n","    y = df.loc[:, [\"Target\"]].copy()\n","\n","    X_selected = sel.fit_transform(X)\n","    # This creates a mask with the columns we're trying to keep\n","    selected_features = sel.get_support()\n","    sel_df = pd.DataFrame(X_selected, columns=X.columns[selected_features])\n","\n","    # The variance threshold object returns an ndarray which removed our index\n","    sel_df.index = y.index\n","    sel_df.loc[:, \"Target\"] = y\n","    df = sel_df\n","    col_names = df.columns\n","\n","    # Apply SMOTE oversampling\n","    X_smote, y_smote = gen_SMOTE_data(df=df)\n","\n","    log_reg = LogisticRegression(solver=\"liblinear\", penalty=\"l2\")\n","\n","    # Fit model on the SMOTE data\n","    log_reg.fit(X_smote, y_smote)\n","\n","    return log_reg, col_names"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:46:13.128635Z","iopub.status.busy":"2023-05-22T06:46:13.127673Z","iopub.status.idle":"2023-05-22T06:46:30.383183Z","shell.execute_reply":"2023-05-22T06:46:30.381625Z","shell.execute_reply.started":"2023-05-22T06:46:13.128604Z"},"trusted":true},"outputs":[],"source":["# where the magic happens\n","train_df = load_train_data()\n","log_model, col_names = best_model(train_df)\n","\n","test_df = load_train_data(filepath = test_data)\n","test_df_cols_removed = test_df.drop(columns=[col for col in test_df if col not in col_names])\n","\n","y_pred = log_model.predict(test_df_cols_removed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:06:50.779040Z","iopub.status.busy":"2023-05-22T07:06:50.778666Z","iopub.status.idle":"2023-05-22T07:06:51.090166Z","shell.execute_reply":"2023-05-22T07:06:51.089023Z","shell.execute_reply.started":"2023-05-22T07:06:50.779011Z"},"trusted":true},"outputs":[],"source":["#merging of relevant files together\n","y_pred_df = pd.DataFrame(y_pred)\n","y_pred_df = y_pred_df.rename(columns={0: 'Target'})\n","y_pred_df = y_pred_df.reset_index(drop=True)\n","\n","test = pd.read_csv(test_data)\n","\n","# df of household head Ids\n","test_hhh_ids = test.loc[test.loc[:, 'parentesco1'] == 1, 'Id']\n","test_hhh_ids_df = pd.DataFrame(test_hhh_ids)\n","test_hhh_ids_df = test_hhh_ids_df.reset_index(drop=True)\n","\n","# df of non-household head Ids\n","test_other_ids = test.loc[test.loc[:, 'parentesco1'] == 0, 'Id']\n","test_other_ids = test_other_ids.reset_index(drop=True)\n","test_other_ids_df = pd.DataFrame(test_other_ids)\n","test_other_ids_df.loc[:, 'Target'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:06:54.108681Z","iopub.status.busy":"2023-05-22T07:06:54.108162Z","iopub.status.idle":"2023-05-22T07:06:54.202454Z","shell.execute_reply":"2023-05-22T07:06:54.201283Z","shell.execute_reply.started":"2023-05-22T07:06:54.108638Z"},"trusted":true},"outputs":[],"source":["test_hhh_ids = test_hhh_ids.reset_index(drop=True)\n","y_pred_df = y_pred_df.reset_index(drop=True)\n","\n","hhh_df = pd.merge(test_hhh_ids_df, y_pred_df, left_index=True, right_index=True)\n","\n","#merged_df = hhh_df.append(test_other_ids_df, ignore_index=True)\n","df_to_submit = pd.concat([hhh_df, test_other_ids_df], sort=False)\n","\n","df_to_submit.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
