{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrew Dunn, Katherine Dumais, Kathryn Link-Oberstar, Lee-Or Bentovim\n",
    "# Summary of initial feature engineering and model testing for checkpoint 2\n",
    "\n",
    "# This analysis runs after we load, clean, and generate additional variables in load_data.py, \n",
    "# and initially explored the data in exploratory_analysis.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import load_data\n",
    "from evaluate_classification import evaluate_classification\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the different dataframes\n",
    "\n",
    "# raw cleaned data\n",
    "df, X_valid, y_valid = load_data.load_train_data()\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.loc[:, 'Target']\n",
    "\n",
    "# Create oversampled dataframes\n",
    "train_X_resampled, train_y_resampled = load_data.gen_oversample_date()\n",
    "X_smote, y_smote = load_data.gen_SMOTE_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "\n",
    "We generate several new variables, derived from our literature review, in load_data.py:\n",
    "\n",
    "- max_education_level: the education level of the person with the highest education level in the household\n",
    "- hh_has_marriage: whether someone in the household is married\n",
    "- hh_max_age: the age of the oldest person in the household\n",
    "- hh_sex_ratio: the ratio of men to women in the household\n",
    "- hh_child_woman_ratio_12: the ratio of children to women in the household, with children defined as being under 12\n",
    "- hh_child_adult_ratio_12: the ratio of children to adults in the household, with children defined as being under 12\n",
    "- hh_child_woman_ratio_19: the ratio of children to women in the household, with children defined as being under 19\n",
    "- hh_child_adult_ratio_19: the ratio of children to adults in the household, with children defined as being under 19\n",
    "- v2a1_log: the logged value of v2a1, the monthly rent payment. We also estimate and impute values into v2a1 when that field is missing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing**\n",
    "\n",
    "We tested variations of four different models: random forest, naive bayes, KNN, and logistic regression in the following notebooks:\n",
    "\n",
    "Below, we demonstrate the best performing variants of those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier(random_state = SEED,\n",
    "                            n_estimators = 1600,\n",
    "                            min_samples_split = 2,\n",
    "                            min_samples_leaf = 1,\n",
    "                            max_features = 'sqrt',\n",
    "                            max_depth = 100,\n",
    "                            bootstrap = False\n",
    ")\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_valid, cm = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "The Naive Bayes algorithm assumes that all features are independent of each other, meaning that the presence or absence of one feature does not affect the probability of another feature being present or absent. The algorithm uses Bayes' theorem to calculate the probabilities of different classes given the observed evidence. \n",
    "\n",
    "The Naive Bayes model is Scalable, can handle large datasets and high-dimensional feature spaces efficiently. However, one of the key assumptions of Naive Bayes is that features are conditionally independent given the class label. *This is almost certainly not true with this data.* \n",
    "Some other imitation of Naive Bayes include:\n",
    "* *Data Scarcity*: Naive Bayes models can suffer when there is not enough data to estimate the probabilities accurately\n",
    "* *Continuous features:* Naive Bayes models work better with categorical data. *We have a lot of bianry and continuous data*\n",
    "* *Imbalanced Datasets*: Naive Bayes models may not perform well on an imbalanced dataset, where the classes are not represented equally. This can lead to poor classification performance, as the Naive Bayes algorithm may be biased towards the majority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df, X_valid, y_valid = ld.load_train_data()\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.loc[:, 'Target']\n",
    "X_true = X_valid\n",
    "y_true = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true,  cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complement Naive Bayes + MinMaxScalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "nb = ComplementNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true, cm = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Naive Bayes Model performed with between 50% and 60% accuracy on training data across various iterations of the model.\n",
    "\n",
    "Main Takeaways: \n",
    "* The model with the best accuracy was the **Bernoulli Naive Bayes with MinMaxScalar**. It had 65% accuracy, and even this model is one of the best performing, its accuracy is primarily due to the fact that it classfies 4, our over represented class, well. The recall values for this model are. Label 1: 0.29 Label 2: 0.4 Label 3: 0.14 Label 4: 0.86\n",
    "* One of the biggest challenged we face in this project is figuring out how to handle the overrepresenation of group 4 in our data. Models that classfiy most data as 4 (the over represented class) may return higher accuracy but may be overall less useful. \n",
    "    * In general, models seemed to behave in 1 of 2 ways:\n",
    "        1. Classifying most values as 4 (i.e. the *Bernoulli Naive Bayes with MinMaxScalar* above), or\n",
    "        2. Split values between 2 and 4 (i.e. *Complement Naive Bayes + MinMaxScalar*)\n",
    "    * **Complement Naive Bayes + MinMaxScalar**: This model has some of the highest accuracy of all the Naive Bayes Models (61%) and is also the best model classfiying 2: Recall: Label 1: 0.0 Label 2: 0.57 Label 3: 0.1 Label 4: 0.79\n",
    "* Overall, MinMaxScalar seemed to be an important step to improve accuracy across the board."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the KNN section, we attempted to build K Nearest Neighbors models with a variety of different inputs using SKlearn's KNN classifiers. We considered the following approach: using uniform or distance based weighting, changing the number of nearest features to select, regularizing the data, and considering a radius based as opposed to nearest neighbors based approach. In general, the results were disappointing, see below table for the full outcomes. I've reproduced the highest performing model below as an example of the model that performed best, however it is useful to note that its recall score shows that even this model is only really correctly classifying data in class 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "neigh.fit(X,y)\n",
    "pred = neigh.predict(X_valid)\n",
    "evaluate_classification(pred,y_valid,cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{c|ccc}\n",
    "\\text{} & \\text{accuracy} & \\text{f1} & \\text{recall [1,2,3,4]} \\\\\n",
    "\\hline\n",
    "k\\_15\\_distance   &      0.60 &  0.53 &     [0.04, 0.1, 0.1, 0.9] \\\\\n",
    "k\\_20\\_distance   &      0.61 &  0.53 &  [0.04, 0.09, 0.12, 0.91] \\\\\n",
    "k\\_3\\_uniform     &      0.53 &  0.52 &  [0.22, 0.23, 0.07, 0.74] \\\\\n",
    "k\\_5\\_uniform     &      0.55 &  0.52 &  [0.06, 0.19, 0.11, 0.79] \\\\\n",
    "k\\_10\\_uniform    &      0.60 &  0.52 &  [0.02, 0.11, 0.04, 0.91] \\\\\n",
    "k\\_10\\_distance   &      0.58 &  0.52 &   [0.04, 0.1, 0.12, 0.86] \\\\\n",
    "k\\_15\\_uniform    &      0.62 &  0.52 &  [0.06, 0.07, 0.04, 0.94] \\\\\n",
    "k\\_20\\_uniform    &      0.62 &  0.52 &  [0.02, 0.06, 0.04, 0.96] \\\\\n",
    "k\\_3\\_distance    &      0.52 &  0.51 &  [0.08, 0.14, 0.18, 0.75] \\\\\n",
    "k\\_5\\_distance    &      0.54 &  0.51 &  [0.06, 0.12, 0.14, 0.79] \\\\\n",
    "re\\_20\\_distance  &      0.46 &  0.50 &  [0.29, 0.33, 0.34, 0.53] \\\\\n",
    "rad\\_3\\_distance  &      0.63 &  0.49 &      [0.0, 0.0, 0.0, 1.0] \\\\\n",
    "rad\\_5\\_distance  &      0.63 &  0.49 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "re\\_15\\_distance  &      0.45 &  0.49 &  [0.31, 0.32, 0.32, 0.53] \\\\\n",
    "rad\\_3\\_uniform   &      0.63 &  0.49 &      [0.0, 0.0, 0.0, 1.0] \\\\\n",
    "re\\_10\\_distance  &      0.45 &  0.49 &  [0.37, 0.28, 0.33, 0.52] \\\\\n",
    "re\\_5\\_distance   &      0.45 &  0.49 &   [0.24, 0.25, 0.3, 0.56] \\\\\n",
    "rad\\_20\\_uniform  &      0.62 &  0.49 &    [0.0, 0.0, 0.01, 0.98] \\\\\n",
    "rad\\_20\\_distance &      0.62 &  0.49 &    [0.0, 0.0, 0.01, 0.98] \\\\\n",
    "rad\\_5\\_uniform   &      0.63 &  0.49 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "re\\_3\\_distance   &      0.44 &  0.48 &  [0.24, 0.24, 0.27, 0.55] \\\\\n",
    "rad\\_10\\_distance &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "rad\\_15\\_distance &      0.62 &  0.48 &     [0.0, 0.0, 0.0, 0.98] \\\\\n",
    "rad\\_15\\_uniform  &      0.62 &  0.48 &     [0.0, 0.0, 0.0, 0.98] \\\\\n",
    "rad\\_10\\_uniform  &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "re\\_3\\_uniform    &      0.43 &  0.47 &  [0.37, 0.32, 0.16, 0.51] \\\\\n",
    "re\\_5\\_uniform    &      0.41 &  0.46 &  [0.35, 0.34, 0.25, 0.47] \\\\\n",
    "re\\_15\\_uniform   &      0.40 &  0.45 &  [0.35, 0.34, 0.26, 0.45] \\\\\n",
    "re\\_20\\_uniform   &      0.40 &  0.45 &  [0.39, 0.36, 0.27, 0.43] \\\\\n",
    "re\\_10\\_uniform   &      0.39 &  0.44 &  [0.43, 0.31, 0.29, 0.42] \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN approach suffered from some issues we were not able to address, leading us to conclude this was not the right model. KNN uses all features by default, but our attempts at selecting only certain features was if anything less successful than using all features. Likewise, an attempt to break out 4 from the rest and then classify the rest between 1,2,3 did worse than a one stage classifier. Finally, when we tried to oversample our classes 1-3, the KNN model continued to predict only class 4. While we will continue to use some of these approaches to better classify our data, it is clear to us that KNN does not provide the best path forwards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic Regression*\\\n",
    "The goal of logistic regression is to find the best parameters of a logistic function that minimizes the difference between the predicted probabilities and the actual outcomes. The logistic function uses the sigmoid function, which maps any real number into a range between 0 and 1, allowing us to interpret the output as a probability. The input features are weighted and combined linearly, and the resulting value is passed through the logistic function to produce a probability. We then use gradient descent to determine what the best classification is for these categories. Regularization helps to improve the generalization performance of the model by balancing the bias-variance trade-off and reducing overfitting.  By penalizing the weights of the input features, regularization encourages the model to focus on the most important features that are most relevant to the target variable. Overfitting occurs when a model learns to fit the training data too closely, including noise and irrelevant features, leading to poor generalization performance on new, unseen data. In this case we will try L2 regularization which adds a penalty term proportional to the square of the weights.\n",
    "\n",
    "One drawback however, is that it assumes linear relationships which based on the diversity of our data, may not be true in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#Train our data with two oversampling methods: SVM SMOTE and random sampling with replacement \n",
    "\n",
    "print(\"With Smote- RFE\")\n",
    "#Recursive feature elimination\n",
    "rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 5, step = 1)\n",
    "rfe.fit(X_smote, y_smote)\n",
    "y_pred = rfe.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "#X_train.iloc[:,[138, 126, 127, 108, 107]]\n",
    "\n",
    "print(\"With Random Selection- RFE\")\n",
    "#Recursive feature elimination\n",
    "rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 5, step = 1)\n",
    "rfe.fit(train_df_resampled, train_y_resampled)\n",
    "y_pred = rfe.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "\n",
    "print(\"With Smote- Variance Threshold\")\n",
    "# get rid of features without a lot of variance \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "a= X_smote.copy()\n",
    "rev_x = sel.fit_transform(a)\n",
    "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, y_smote)\n",
    "\n",
    "y_pred = reg.predict(sel.transform(X_valid.copy()))\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "print(\"With Random Selection-Variance threshold\")\n",
    "\n",
    "# get rid of features without a lot of variance \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "rev_x = sel.fit_transform(train_df_resampled.copy())\n",
    "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, train_y_resampled)\n",
    "a= X_valid.copy()\n",
    "y_pred = reg.predict(sel.transform(a))\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, if we reference the logistic regression function, without overweighting we only classify the majority class 4 (please see regression document). With overweighting and feature selection methods, we get our best predictors using regularized logistic regression in terms of classification accuracy and F1 score. But given the diversity of the results, it is clear feature selection as opposed to logistic regression usage is the most important part of this work. It is unclear if regularized logistic regression will be the best indicator going forward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion **"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Next Steps **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "costa-rican-household-poverty-level-predic-mFobo9k6-py3.11",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
