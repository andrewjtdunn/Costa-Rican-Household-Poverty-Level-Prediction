{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression For Costa Rican Poverty Prediction\n",
    "\n",
    "### Outline\n",
    "**1. Project Setup** \\\n",
    "*1.1 Load Data and Packages* \\\n",
    "*1.2 Data Cleaning* \\\n",
    "*1.3 Oversampling* \\\n",
    "\\\n",
    "**2. What is a logistic regression?** \\\n",
    "*2.1 Summary of Approach* \\\n",
    "\\\n",
    "**3. Models** \\\n",
    "*3.1 Basic Models* \\\n",
    "*3.2 Improving Model Performance* \\\n",
    "\\\n",
    "**4. Findings, Limitations and Next Steps**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, recall_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "current_dir = os.getcwd()\n",
    "print(os.getcwd())\n",
    "#parent_dir = os.path.dirname(current_dir)\n",
    "#os.chdir(parent_dir)\n",
    "import load_data as ld\n",
    "df, X_valid, y_valid, train_indices, valid_indices= ld.load_train_data()\n",
    "from evaluate_classification import evaluate_classification\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X_valid, y_valid, train_indices, valid_indices= ld.load_train_data()\n",
    "print(train_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Create oversampling:**\\\n",
    "We are looking to better predict Costa Rican Poverty in the following categories: \n",
    "- 1 = extreme poverty\n",
    "- 2 = moderate poverty\n",
    "- 3 = vulnerable households\n",
    "- 4 = non vulnerable households\n",
    "\n",
    "However, as demonstrated in our exploratory analysis, the majority of the data (66%) comes from class 4. Thus, a strong training model is most likely to predict the majority class rather than give accurate weights and biases as to reflect the different categories. \n",
    "\n",
    "As such we attempt to do over sampling- with random oversampling, SMOTE and ADASYM. We try random sampling with replacement as well as SVM SMOTE (Support Vector Machine Synthetic Minority Over-sampling Technique), which creates synthetic samples by randomly sampling the characteristics from occurrences in the minority class. The SVM class is designed for small and complex datasets. For SVM SMOTE, a neighborhood is defined using the parameter m_neighbors to decide if a sample is in danger, safe, or noise.\n",
    "\n",
    "This gives us a number of different datasets to work with, so we can see how oversampling can impact our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[: , :-1]\n",
    "y = df.loc[:, 'Target']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "train_df_resampled, train_y_resampled = ld.gen_oversample_data(train, seed = 12)\n",
    "\n",
    "print(\"original y train\", sorted(Counter(y_train).items()))\n",
    "print( \"Random Sampling\", sorted(Counter(train_y_resampled).items()))\n",
    "\n",
    "X_smote, y_smote = ld.gen_SMOTE_data(train, seed = 12)\n",
    "print(\"SMOTE\", sorted(Counter(train_y_resampled).items()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Logistic Regression? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a type of supervised learning algorithm used for classification problems which uses a logistic function to model the probability of a binary or categorical outcome. In our case, our 1-4 levels of poverty produce categorical variables. \n",
    "\n",
    "The goal of logistic regression is to find the best parameters of a logistic function that minimizes the difference between the predicted probabilities and the actual outcomes. The logistic function uses the sigmoid function, which maps any real number into a range between 0 and 1, allowing us to interpret the output as a probability. The input features are weighted and combined linearly, and the resulting value is passed through the logistic function to produce a probability. We then use gradient descent to determine what the best classification is for these categories. \n",
    "\n",
    "One drawback however, is that it assumes linear relationships which based on the diversity of our data, may not be true in this case. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is Regularization\n",
    "Regularization helps to improve the generalization performance of the model by balancing the bias-variance trade-off and reducing overfitting.  By penalizing the weights of the input features, regularization encourages the model to focus on the most important features that are most relevant to the target variable. Overfitting occurs when a model learns to fit the training data too closely, including noise and irrelevant features, leading to poor generalization performance on new, unseen data. In this case we will try L2 regularization which adds a penalty term proportional to the square of the weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models\n",
    "*3.1 Basic Models*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit the model with data\n",
    "\n",
    "reg = LogisticRegression(solver='lbfgs', penalty=None, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n",
    "\n",
    "print(\"Random Sampling\")\n",
    "reg = LogisticRegression(solver='lbfgs', penalty=None, max_iter=1000).fit(train_df_resampled, train_y_resampled)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n",
    "\n",
    "print(\"SVM SMOTE\")\n",
    "reg = LogisticRegression(solver='lbfgs', penalty=None, max_iter=1000).fit(X_smote, y_smote)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1 With Regularization*\n",
    "\n",
    "As demonstrated below, regularization is crucial to the success of these models. It also allows us to use a better solver for smaller datasets: liblinear. It also has a substantially higher F1 score. However, the randomly selected do a better job of classifying the data into the correct catergory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit the model with data\n",
    "\n",
    "reg = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "print(\"Random Sampling\")\n",
    "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(train_df_resampled, train_y_resampled)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "print(\"SVM SMOTE\")\n",
    "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(X_smote, y_smote)\n",
    "y_pred = reg.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.2 Improve Model Performance*\n",
    "\n",
    "Adding Feature selection to these models as shown below does not seem to increase accuracy in a meaningful way - Particularly in characterization of 1's and two's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_models():\n",
    "    print(\"Random forest\")\n",
    "    reg = RandomForestClassifier()\n",
    "    reg.fit(X_smote, y_smote)\n",
    "    y_pred = reg.predict(X_valid)\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "    print(\"Random forest -RFE\")\n",
    "    reg = RandomForestClassifier()\n",
    "    reg.fit(train_df_resampled, train_y_resampled)\n",
    "    y_pred = reg.predict(X_valid)\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "\n",
    "    print(\"With Smote- RFE\")\n",
    "    #Recursive feature elimination\n",
    "    rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 6, step = 1)\n",
    "    rfe.fit(X_smote, y_smote)\n",
    "    y_pred = reg.predict(X_valid)\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "    print(\"Features selected:With Smote- RFE\")\n",
    "    print(X_train.iloc[:,[64,138,126,127,107,108,6]].columns)\n",
    "\n",
    "    print(\"With Random Selection- RFE\")\n",
    "    #Recursive feature elimination\n",
    "    rfe = RFE(estimator=LogisticRegression(solver='liblinear', penalty='l2'),n_features_to_select = 6, step = 1)\n",
    "    rfe.fit(train_df_resampled, train_y_resampled)\n",
    "    y_pred = reg.predict(X_valid)\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "    print(\"Features selected:With Random Selection- RFE\")\n",
    "    print(X_train.iloc[:,[36,37,42,45,107,108]].columns)\n",
    "\n",
    "\n",
    "    print(\"With Smote- Variance Threshold\")\n",
    "    # get rid of features without a lot of variance \n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "    a= X_smote.copy()\n",
    "    rev_x = sel.fit_transform(a)\n",
    "    reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, y_smote)\n",
    "\n",
    "    y_pred = reg.predict(sel.transform(X_valid.copy()))\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "\n",
    "    print(\"With Random Selection-Variance threshold\")\n",
    "\n",
    "    # get rid of features without a lot of variance \n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "    rev_x = sel.fit_transform(train_df_resampled.copy())\n",
    "    reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, train_y_resampled)\n",
    "    a= X_valid.copy()\n",
    "    y_pred = reg.predict(sel.transform(a))\n",
    "    evaluate_classification(y_pred, y_true = y_valid, labels=[1,2,3,4], cm = True)\n",
    "Run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(solver='liblinear', penalty='l2')\n",
    "\n",
    "bagging_classifier_rf = BaggingClassifier(\n",
    "    base_estimator=reg,\n",
    "    n_estimators=10,\n",
    "    random_state=42)\n",
    "\n",
    "bagging_classifier_rf.fit(X_smote, y_smote)\n",
    "bc_rf_predictions = bagging_classifier_rf.predict(X_valid)\n",
    "evaluate_classification(bc_rf_predictions, y_valid, l=[1,2,3,4], cm=True, return_vals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "rev_x = sel.fit_transform(X_smote.copy())\n",
    "reg = RandomForestClassifier(random_state = 12,\n",
    "                        n_estimators = 1600,\n",
    "                        min_samples_split = 2,\n",
    "                        min_samples_leaf = 1,\n",
    "                        max_features = 'sqrt',\n",
    "                        max_depth = 100,\n",
    "                        bootstrap = False).fit(rev_x, y_smote)\n",
    "a= X_valid.copy()\n",
    "y_pred = reg.predict(sel.transform(a))\n",
    "evaluate_classification(y_pred, y_true = y_valid, l=[1,2,3,4], cm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "a= X_smote.copy()\n",
    "rev_x = sel.fit_transform(a)\n",
    "reg = LogisticRegression(solver='liblinear', penalty='l2').fit(rev_x, y_smote)\n",
    "\n",
    "bagging_classifier_rf = BaggingClassifier(\n",
    "    base_estimator=reg,\n",
    "    n_estimators=10,\n",
    "    random_state=42)\n",
    "\n",
    "bagging_classifier_rf.fit(rev_x, y_smote)\n",
    "bc_rf_predictions = bagging_classifier_rf.predict(sel.transform(X_valid.copy()))\n",
    "evaluate_classification(bc_rf_predictions, y_valid, l=[1,2,3,4], cm=True, return_vals=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Findings, Limitations and Next Steps**\n",
    "\n",
    "Our models regardless of oversampling are fairly accurate at predicting 4, but need more data to provide any meaningful accuracy using logistic regression to predict the lower categories at any meaningful amount of accuracy. While, if we reference the logistic regression function, without overweighting we only classify the majority class 4. With overweighting and feature selection methods, we get our best predictors using regularized logistic regression in terms of classification accuracy and F1 score. But given the diversity of the results, and column selection in model development, it is clear feature selection as opposed to logistic regression usage is the most important part of this work. It is unclear if regularized logistic regression will be the best indicator going forward.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
