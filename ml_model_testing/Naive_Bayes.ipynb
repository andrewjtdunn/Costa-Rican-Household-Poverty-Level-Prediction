{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model for Costa Rican Poverty Level Predicion\n",
    "\n",
    "### Outline\n",
    "**1. Project Setup** \\\n",
    "\\\n",
    "**2. What is a Naive Bayes Model?** \\\n",
    "*2.1 Strengths and Weaknesses of a Naive Bayes model for this project* \\\n",
    "*2.2 Summary of Approach* \\\n",
    "*2.3 Summary of Findings* \\\n",
    "\\\n",
    "**3. Models** \\\n",
    "*3.1 Basic Models* \\\n",
    "*3.2 Improving Model Performance* \\\n",
    "\\\n",
    "**4. Conclusions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data & Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, recall_score, classification_report, precision_recall_fscore_support, balanced_accuracy_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "SEED = 12\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import load_data as ld\n",
    "from evaluate_classification import evaluate_classification\n",
    "df, X_valid, y_valid, train_indices, valid_indices = ld.load_train_data(filepath = 'Kaggle_download/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X and y for training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.loc[:, 'Target']\n",
    "y_true = y_valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary of Column Classfication by Naive Bayes Model Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'rez_esc' not in Gaussian, 'parentesco1' not in bernoulli\n",
    "nb_datatypes = {'gaussian' :['agesq', 'dependency' , 'edjefa', 'edjefe', 'escolari', 'tamhog', 'max_education_level', 'hh_max_age'], \n",
    "                'bernoulli' : ['pisomoscer', 'paredzocalo', 'techoentrepiso', 'elimbasu5', 'tipovivi5', 'tipovivi2', \n",
    "                               'instlevel5', 'instlevel3', 'instlevel7', 'coopele', 'planpri', 'v14a', 'parentesco9', \n",
    "                               'parentesco10', 'dis', 'estadocivil4', 'parentesco8', 'female', 'eviv1', 'eviv3', 'eviv2', \n",
    "                               'estadocivil2', 'parentesco6', 'estadocivil1', 'male', 'estadocivil3', 'mobilephone', \n",
    "                               'parentesco7', 'pisonotiene', 'abastaguano', 'parentesco11', 'parentesco12', 'pisonatur', 'pisocemento', \n",
    "                               'pisoother', 'pisomadera', 'paredblolad', 'paredfibras', 'paredother', 'paredpreb', 'pareddes', 'paredmad', \n",
    "                               'paredzinc', 'techozinc', 'techocane', 'techootro', 'etecho1', 'etecho3', 'etecho2', 'elimbasu2', 'elimbasu3', \n",
    "                               'elimbasu1', 'elimbasu4', 'elimbasu6', 'estadocivil5', 'estadocivil7', 'parentesco3', 'parentesco5', \n",
    "                               'parentesco2', 'parentesco4', 'cielorazo', 'computer', 'refrig', 'television', 'epared1', 'epared3', \n",
    "                               'epared2', 'abastaguadentro', 'abastaguafuera', 'estadocivil6', 'instlevel4', 'instlevel2', 'instlevel6', \n",
    "                               'energcocinar2', 'energcocinar3', 'energcocinar4', 'noelec', 'instlevel1', 'energcocinar1', 'sanitario1', \n",
    "                               'hacdor', 'hacapo', 'tipovivi1', 'instlevel9', 'tipovivi4', 'lugar4', 'lugar1', 'lugar2', 'lugar5', 'lugar6', \n",
    "                               'lugar3', 'tipovivi3', 'sanitario3', 'sanitario5', 'sanitario6', 'sanitario2', 'instlevel8', 'area1', 'v18q', 'area2',\"hh_has_marriage\"],\n",
    "                               'multinomial' : [\"r4m2\", \"r4m1\", \"hhsize\", \"r4h2\", \"r4h1\", \"hogar_nin\", \"tamviv\", \"v18q1\", \"r4t2\", \"r4t1\", \n",
    "                                \"r4m3\", \"r4h3\", \"r4t3\", \"meaneduc\", \"qmobilephone\", \"hogar_total\", \"overcrowding\", \"hogar_mayor\", \n",
    "                                \"bedrooms\", \"hogar_adul\"], 'sq_vals' : ['SQBdependency', 'SQBmeaned', 'SQBage', 'SQBhogar_nin', \n",
    "                                                                        'SQBhogar_total', 'SQBovercrowding', 'SQBedjefe', 'SQBescolari', \"hh_sex_ratio\", \"hh_child_woman_ratio\",\"hh_child_adult_ratio\"]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Naive Bayes Model?\n",
    "\n",
    "The Naive Bayes algorithm assumes that all features are independent of each other, meaning that the presence or absence of one feature does not affect the probability of another feature being present or absent. The algorithm uses Bayes' theorem to calculate the probabilities of different classes given the observed evidence. Bayes' theorem allows us to update our beliefs about the probability of a hypothesis (such as the class of a data point) based on new evidence (such as the features of the data point).\n",
    "\n",
    "In this case, the Naive Bayes model calculates the probability of a household belonging to 1 of 4 target classes:\n",
    "\n",
    "- 1 = extreme poverty\n",
    "- 2 = moderate poverty\n",
    "- 3 = vulnerable households\n",
    "- 4 = non vulnerable households"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Strengths and Weaknesses of a Naive Bayes model for this project\n",
    "\n",
    "##### The benefits of a Naive Bayes model for this project are:\n",
    "- **Scalability:** Naive bays can handle large datasets and high-dimensional feature spaces efficiently, as the computational complexity of training is linear with the number of features. We have a lot of features in our dataset, and this model is able to parse through many features quickly.\n",
    "\n",
    "- **Speed:** Naive Bayes models are fast to train and predict. This allows us to run many different versions and experiment with different structures and parameters.\n",
    "\n",
    "##### The drawbacks of a Naive Bayes model for this project are:\n",
    "- **Independence assumption:** Naive Bayes models assume that features are conditionally independent given the class label. *This is almost certainly not true with this data.*\n",
    "\n",
    "- **Data scarcity:** The performance of Naive Bayes models can suffer when there is not enough data to estimate the probabilities accurately. *We know we have a data scarcity problem with this dataset*\n",
    "\n",
    "- **Continuous features:** Naive Bayes models work better with categorical data. *We have a lot of bianry and continuous data*\n",
    "\n",
    "- **Imbalanced Datasets**: Naive Bayes models may not perform well on an imbalanced dataset, where the classes are not represented equally. This can lead to poor classification performance, as the Naive Bayes algorithm may be biased towards the majority class.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Summary of Approach\n",
    "\n",
    "**There are three main types of Naive Bayes models:**\n",
    "\n",
    "1. Gaussian Naive Bayes: Gaussian Naive Bayes assumes that the continuous data follows a normal distribution. In our dataset this includes things like age and years of education.\n",
    "\n",
    "2. Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age.\n",
    "\n",
    "3. Bernoulli Naive Bayes: Bernoulli Naive Bayes is also used for discrete data, the presence or absence of a certain attribute or feature. In our dataset this includes all the binary variables such as whether or not the floors are good and whether or not the dwelling has a toilet.\n",
    "\n",
    "*Each of these models can be run in isolation or in conjunction*\n",
    "\n",
    "There is also the Complement Naive Bayes Model which is a variation of the standard Naive Bayes algorithm developed to address the issue of imbalanced datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Summary of Findings\n",
    "\n",
    "*We explored the following models. Accuracy refers to performance on training data.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Technique / Model                                                                  | Accuracy |\n",
    "|------------------------------------------------------------------------------------|----------|\n",
    "| Standard Gaussian Naive Bayes                                                      | 52%      |\n",
    "| Gaussian NB on Continuous Features ONLY                                            | 52%      |\n",
    "| Standard Multinomial Naive Bayes                                                   | 48%      |\n",
    "| Multinomial NB on Count Features ONLY                                              | 60%      |\n",
    "| Standard Bernoulli Naive Bayes                                                     | 60%      |\n",
    "| Bernoulli NB on Binary Features ONLY                                               | 59%      |\n",
    "| MinMaxScalar on Bernoulli Naive Bayes                                              | 64%      |\n",
    "| MinMaxScalar on Multinomial Naive Bayes                                            | 19%      |\n",
    "| MinMaxScalar on Bernoulli Naive Bayes                                              | 60%      |\n",
    "| Model Ensembling (with MinMaxScalar)                                               | 59%      |\n",
    "| Complement Naive Bayes                                                             | 51%      |\n",
    "| Complement NB + MinMaxScaling                                                      | 61%      |\n",
    "| Oversampling with SMOTE + MinMax Scalar with Multinomial Naive Bayes               | 47%      |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models\n",
    "\n",
    "Each of the models is explained below, with its accompanying accuracy, F1 score, Recall and Confusion Matrix.\n",
    "\n",
    "### 3.1 Basic Models\n",
    "\n",
    "#### Basic Gaussian Naive Bayes Model on Full Dataset\n",
    "Gaussian Naive Bayes: Gaussian Naive Bayes assumes that the continuous data follows a normal distribution. In our dataset this includes things like age and years of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True) #evaluate_classification(y_pred, cm = True) to see Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes Model on Continuous Features ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_g = X[nb_datatypes['gaussian']]\n",
    "X_valid_g = X_valid[nb_datatypes['gaussian']]\n",
    "gnb = GaussianNB()\n",
    "nb.fit(X_g, y)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Multinomial Naive Bayes on Full Dataset\n",
    "Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes on Count features ONLY\n",
    "Multinomial Naive Bayes: Multinomial Naive Bayes is used for discrete data. In our dataset this includes things like number of people in the household and number of people avode or below a certain age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m = X[nb_datatypes['multinomial']]\n",
    "X_valid_m = X_valid[nb_datatypes['multinomial']]\n",
    "gnb = MultinomialNB()\n",
    "nb.fit(X_m, y)\n",
    "y_pred = nb.predict(X_valid_m)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Bernoulli Naive Bayes on Full Dataset\n",
    "Bernoulli Naive Bayes: Bernoulli Naive Bayes is also used for discrete data, the presence or absence of a certain attribute or feature. In our dataset this includes all the binary variables such as whether or not the floors are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X, y)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes on binary features ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = X[nb_datatypes['bernoulli']]\n",
    "X_valid_b = X_valid[nb_datatypes['bernoulli']]\n",
    "bnb = BernoulliNB()\n",
    "nb.fit(X_b, y)\n",
    "y_pred = nb.predict(X_valid_b)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Techniques to improve our model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScalar\n",
    " MinMaxScaler is used to scale the numerical features of the dataset. Scaling is a common preprocessing step that can help improve the performance of certain machine learning algorithms, including the Multinomial Naive Bayes.\n",
    "\n",
    "- Feature scaling: MinMaxScaler scales the numerical features by subtracting the minimum value of the feature and dividing by the range (maximum value - minimum value) for each feature. This ensures that all the features have the same scale.\n",
    "\n",
    "- Handling non-negative data: MNB assumes that the input features follow a multinomial distribution, which requires non-negative values. By scaling the numerical features using MinMaxScaler, you ensure that all values are non-negative, satisfying the input requirements for MNB.\n",
    "\n",
    "- Equal weighting: When features have different scales, the algorithm might give more importance to features with larger values. Scaling the features ensures that they all have equal weight in the model, preventing any unintended biases based on the original scale of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Multinomial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true, cm=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Gaussian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScalar on Bernoulli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensembling (with MinMaxScalar)\n",
    "\n",
    "More than on Naive Bayes models can be combined when our features are of different data types. This technique is known as \"model stacking\" or \"ensembling.\" In this approach, we train separate Naive Bayes classifiers for different types of features and then combine their predictions to make a final decision. An ensemble model is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Gaussian features\n",
    "gaussian_columns = nb_datatypes['gaussian']\n",
    "\n",
    "# Bernoulli features\n",
    "bernoulli_columns = nb_datatypes['bernoulli']\n",
    "\n",
    "# Multinomial features\n",
    "multinomial_columns = nb_datatypes['multinomial']\n",
    "\n",
    "# Create Naive Bayes models\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit models on the respective features\n",
    "gnb.fit(X.iloc[:, :len(gaussian_columns)], y)\n",
    "bnb.fit(X.iloc[:, len(gaussian_columns):len(gaussian_columns) + len(bernoulli_columns)], y)\n",
    "mnb.fit(X.iloc[:, -len(multinomial_columns):], y)\n",
    "\n",
    "# Create an ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('gnb', gnb), ('bnb', bnb), ('mnb', mnb)], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble_model.fit(X, y)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = ensemble_model.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes \n",
    "\n",
    "Complement Naive Bayes (CNB) is an extension of Multinomial Naive Bayes that is particularly suited for imbalanced datasets. It is based on the assumption that the complement of a class has a similar distribution to that of the class itself. CNB uses a frequency-based approach, similar to MultinomialNB, but instead of counting the frequency of each feature in each class, it counts the frequency of each feature in the complement of each class.\n",
    "\n",
    "In order to use CNB with different data types, we can follow a similar approach as with MultinomialNB. First, we need to separate the data into different types, such as numerical, binary, and count data. We can then use a custom transformer to select the relevant features and transform them appropriately. For example, we can use a StandardScaler for numerical data, a BinaryEncoder for binary data, and a CountVectorizer for count data.\n",
    "\n",
    "Here's an example of how to use CNB with different data types in scikit-learn:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Complement Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb = ComplementNB()\n",
    "cnb.fit(X, y)\n",
    "y_pred = cnb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complement Naive Bayes + MinMaxScalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)\n",
    "nb = ComplementNB()\n",
    "nb.fit(X_scaled, y)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "y_pred = nb.predict(X_valid_scaled)\n",
    "evaluate_classification(y_pred, y_true, cm = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling with SMOTE + MinMax Scalar with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_scaled = scaler.fit_transform(X)\n",
    "smote = SMOTE(random_state=10)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X, y)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_true, cm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Naive Bayes Model performed with between 50% and 60% accuracy on training data across various iterations of the model.\n",
    "\n",
    "Main Takeaways: \n",
    "- The model with the best accuracy was the **Bernoulli Naive Bayes with MinMaxScalar**. It had 64% accuracy, and even this model is one of the best performing, its accuracy is primarily due to the fact that it classfies 4, our over represented class, well. The recall values for this model are. Label 1: 0.29 Label 2: 0.38 Label 3: 0.14 Label 4: 0.86\n",
    "- One of the biggest challenged we face in this project is figuring out how to handle the overrepresenation of group 4 in our data. Models that classfiy most data as 4 (the over represented class) may return higher accuracy but may be overall less useful. \n",
    "    - In general, models seemed to behave in 1 of 2 ways:\n",
    "        1. Classifying most values as 4 (i.e. the *Bernoulli Naive Bayes with MinMaxScalar* above), or\n",
    "        2. Split values between 2 and 4 (i.e. *Complement Naive Bayes + MinMaxScalar*)\n",
    "    - **Complement Naive Bayes + MinMaxScalar**: This model has some of the highest accuracy of all the Naive Bayes Models (61%) and is also the best model classfiying 2: Label 2: 0.57 recall and Label 4: 0.79 recall.\n",
    "- Overall, MinMaxScalar seemed to be an important step to improve accuracy across the board.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loops \n",
    "df, X_valid, y_valid, train_indices, valid_indices = ld.load_train_data(filepath=\"Kaggle_download/train.csv\", seed=SEED)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = df.iloc[:, :-1]\n",
    "y_train = df.loc[:, 'Target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of the Box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_valid, cm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_valid, cm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state = SEED,\n",
    "                            n_estimators = 1600,\n",
    "                            min_samples_split = 2,\n",
    "                            min_samples_leaf = 1,\n",
    "                            max_features = 'sqrt',\n",
    "                            max_depth = 100,\n",
    "                            bootstrap = False\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_valid, cm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='liblinear', penalty='l2')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_valid)\n",
    "evaluate_classification(y_pred, y_valid, cm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "loops.loop_model(nb, df, train_indices, valid_indices, scaler=scaler, \n",
    "               oversample=ld.gen_SMOTE_data, var_thresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "loops.loop_model(nb, df, train_indices, valid_indices, scaler=scaler, \n",
    "               oversample=ld.gen_SMOTE_data, var_thresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nb = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "loops.loop_model(nb, df, train_indices, valid_indices, scaler=scaler, \n",
    "               oversample=ld.gen_SMOTE_data, var_thresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode target classes in the DataFrame\n",
    "df['Target'] = df['Target'].replace({1: 1, 2: 1, 3: 0, 4: 0})\n",
    "\n",
    "# Recode target classes in the validation data\n",
    "y_valid['Target'] = y_valid['Target'].replace({1: 1, 2: 1, 3: 0, 4: 0})\n",
    "nb = ComplementNB()\n",
    "loops.loop_model(nb, df, train_indices, valid_indices, scaler=scaler, \n",
    "               oversample=ld.gen_SMOTE_data, var_thresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_results = loops.loop_model(clf,df,train_indices,valid_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capp-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
