{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model for Costa Rican Poverty Level Predicion\n",
    "\n",
    "### Outline\n",
    "**1. Project Setup** \\\n",
    "\\\n",
    "**2. What is a KNN Model?** \\\n",
    "\\\n",
    "**3. Models** \\\n",
    "*3.1 Basic Models* \\\n",
    "*3.2 Radius Neighbors Models* \\\n",
    "*3.3 Oversampled Models* \\\n",
    "\\\n",
    "**4. Results and Next Steps**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils import compute_class_weight\n",
    "import load_data as ld\n",
    "import evaluate_classification as ec\n",
    "\n",
    "\n",
    "df, X_valid, y_valid = ld.load_train_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X and y using oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(df.drop(columns='Target'), df.loc[:,'Target'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KNN Model\n",
    "\n",
    "The KNN algorithm is a common supervised learning classification algorithm. Class labels are assigned by plurality vote of the K nearest neighbors (hence the name). Two common methods for this plurality vote system are uniform (1 point 1 vote), and weighted, where points closer to our target point get more weight on their \"vote\". KNN is a good fit for smaller datasets like the one we have, but it does suffer because it values all columns equally when computing distance between points, suffering from the curse of dimensionality and being prone to overfitting.\n",
    "\n",
    "KNN is a relatively simple model, requiring only two hyperparameters, k and a distance metric, although we use three below when substituting between uniform and weighted weighting as well. \n",
    "\n",
    "In this case, the KNN models make a prediction, using the K nearest neighbors of which of these categories best describes a household:\n",
    "\n",
    "- 1 = extreme poverty\n",
    "- 2 = moderate poverty\n",
    "- 3 = vulnerable households\n",
    "- 4 = non vulnerable households"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic KNN Models\n",
    "\n",
    "Below we run our first set of KNN models, looping through selected k values from 3-20 as well as alternating between uniform and distance based weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "scores = {}\n",
    "for num in [3,5,10,15,20]:\n",
    "    for choice in ['uniform','distance']:\n",
    "        neigh = KNeighborsClassifier(n_neighbors=num,weights=choice)\n",
    "        neigh.fit(df.drop(columns='Target'),df.loc[:,'Target'])\n",
    "        preds[f'k_{num}_{choice}'] = neigh.predict(X_valid)\n",
    "        print(f'k_{num}_{choice}')\n",
    "        accuracy, f1, recall = ec.evaluate_classification(preds[f'k_{num}_{choice}'],y_valid,cm=True,return_vals=True)\n",
    "        scores[f'k_{num}_{choice}'] = {'accuracy': accuracy, 'f1': f1, 'recall': recall}\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Radius Neighbors Models\n",
    "\n",
    "Below we run a set of radius sizes and weighting options on a Radius Neighbors model, which is similar to KNN except rather than specifying the number of neighbors we consider all neighbors within a certain radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in [10,50,100,150,200]:\n",
    "    for choice in ['uniform','distance']:\n",
    "        neigh = RadiusNeighborsClassifier(radius=num, weights=choice, outlier_label='most_frequent')\n",
    "        neigh.fit(df.drop(columns='Target'),df.loc[:,'Target'])\n",
    "        preds[f'rad_{num}_{choice}'] = neigh.predict(X_valid)\n",
    "        print(f'rad_{num}_{choice}')\n",
    "        accuracy, f1, recall = ec.evaluate_classification(preds[f'rad_{num}_{choice}'],y_valid,cm=True,return_vals=True)\n",
    "        scores[f'rad_{num}_{choice}'] = {'accuracy': accuracy, 'f1': f1, 'recall': recall}\n",
    "        print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Oversampled Models\n",
    "\n",
    "Finally, we come back to our original KNN form, but instead of using our original dataset, we use the oversampled version we got through the SMOTE process we ran at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in [3,5,10,15,20]:\n",
    "    for choice in ['uniform','distance']:\n",
    "        neigh = KNeighborsClassifier(n_neighbors=num,weights=choice)\n",
    "        neigh.fit(X_train_res,y_train_res)\n",
    "        preds[f're_{num}_{choice}'] = neigh.predict(X_valid)\n",
    "        print(f're_{num}_{choice}')\n",
    "        accuracy, f1, recall = ec.evaluate_classification(preds[f're_{num}_{choice}'],y_valid,cm=True,return_vals=True)\n",
    "        scores[f're_{num}_{choice}'] = {'accuracy': accuracy, 'f1': f1, 'recall': recall}\n",
    "        print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the dictionary of dictionaries\n",
    "df3 = pd.DataFrame.from_dict(scores, orient='index')\n",
    "df3.sort_values(by = 'f1', ascending=False,inplace=True)\n",
    "\n",
    "print(df3.to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{c|ccc}\n",
    "\\text{} & \\text{accuracy} & \\text{f1} & \\text{recall} \\\\\n",
    "\\hline\n",
    "k\\_15\\_distance    &      0.60 &  0.53 &     [0.04, 0.1, 0.1, 0.9] \\\\\n",
    "k\\_20\\_distance    &      0.61 &  0.53 &  [0.04, 0.09, 0.12, 0.91] \\\\\n",
    "k\\_3\\_uniform      &      0.53 &  0.52 &  [0.22, 0.23, 0.07, 0.74] \\\\\n",
    "rad\\_150\\_uniform  &      0.56 &  0.52 &  [0.08, 0.14, 0.07, 0.83] \\\\\n",
    "k\\_5\\_uniform      &      0.55 &  0.52 &  [0.06, 0.19, 0.11, 0.79] \\\\\n",
    "k\\_10\\_uniform     &      0.60 &  0.52 &  [0.02, 0.11, 0.04, 0.91] \\\\\n",
    "k\\_10\\_distance    &      0.58 &  0.52 &   [0.04, 0.1, 0.12, 0.86] \\\\\n",
    "k\\_15\\_uniform     &      0.62 &  0.52 &  [0.06, 0.07, 0.04, 0.94] \\\\\n",
    "k\\_20\\_uniform     &      0.62 &  0.52 &  [0.02, 0.06, 0.04, 0.96] \\\\\n",
    "rad\\_150\\_distance &      0.57 &  0.52 &   [0.04, 0.1, 0.08, 0.86] \\\\\n",
    "k\\_5\\_distance     &      0.54 &  0.51 &  [0.06, 0.12, 0.14, 0.79] \\\\\n",
    "rad\\_200\\_distance &      0.56 &  0.51 &  [0.02, 0.12, 0.08, 0.83] \\\\\n",
    "rad\\_200\\_uniform  &      0.55 &  0.51 &  [0.04, 0.16, 0.07, 0.82] \\\\\n",
    "rad\\_50\\_uniform   &      0.60 &  0.51 &  [0.04, 0.07, 0.04, 0.93] \\\\\n",
    "rad\\_100\\_uniform  &      0.57 &  0.51 &  [0.12, 0.08, 0.05, 0.86] \\\\\n",
    "k\\_3\\_distance     &      0.52 &  0.51 &  [0.08, 0.14, 0.18, 0.75] \\\\\n",
    "rad\\_100\\_distance &      0.57 &  0.50 &  [0.04, 0.07, 0.05, 0.88] \\\\\n",
    "rad\\_50\\_distance  &      0.60 &  0.50 &  [0.02, 0.05, 0.04, 0.93] \\\\\n",
    "re\\_20\\_distance   &      0.46 &  0.50 &  [0.29, 0.33, 0.34, 0.53] \\\\\n",
    "re\\_5\\_distance    &      0.45 &  0.49 &   [0.24, 0.25, 0.3, 0.56] \\\\\n",
    "re\\_10\\_distance   &      0.45 &  0.49 &  [0.37, 0.28, 0.33, 0.52] \\\\\n",
    "re\\_15\\_distance   &      0.45 &  0.49 &  [0.31, 0.32, 0.32, 0.53] \\\\\n",
    "rad\\_10\\_distance  &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "rad\\_10\\_uniform   &      0.63 &  0.48 &     [0.0, 0.0, 0.0, 0.99] \\\\\n",
    "re\\_3\\_distance    &      0.44 &  0.48 &  [0.24, 0.24, 0.27, 0.55] \\\\\n",
    "re\\_3\\_uniform     &      0.43 &  0.47 &  [0.37, 0.32, 0.16, 0.51] \\\\\n",
    "re\\_5\\_uniform     &      0.41 &  0.46 &  [0.35, 0.34, 0.25, 0.47] \\\\\n",
    "re\\_15\\_uniform    &      0.40 &  0.45 &  [0.35, 0.34, 0.26, 0.45] \\\\\n",
    "re\\_20\\_uniform    &      0.40 &  0.45 &  [0.39, 0.36, 0.27, 0.43] \\\\\n",
    "re\\_10\\_uniform    &      0.39 &  0.44 &  [0.43, 0.31, 0.29, 0.42] \\\\\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on the above table, the highest performing models are weighted KNN models (represented with a k), although some of the larger radius models work well. The resampling models (represented with re), performed worst of all, as the resampling process combined with the limitations of KNN's curse of dimensionality to get models that predicted all four classes very poorly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our conclusions here, especially when compared with the other model types our group built, KNN was clearly the wrong approach to this problem, and as we take next steps we will be using alternative modelling approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
