{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Andrew Dunn, Katherine Dumais, Kathryn Link-Oberstar, Lee-Or Bentovim\n",
                "# Initial exploratory analysis for Costa Rican household poverty level prediction\n",
                "\n",
                "# This analysis runs after we load, clean, and generate additional variables in load_data.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from load_data import load_train_data \n",
                "import json\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.linear_model import LinearRegression\n",
                "import statsmodels.api as sm\n",
                "import matplotlib.pyplot as plt\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "X_train, X_valid, y_train, y_valid = load_train_data()\n",
                "\n",
                "# combine the training datasets into one\n",
                "df = X_train.merge(y_train, left_index = True, right_index = True)\n",
                "\n",
                "# Load variable descriptions\n",
                "with open('var_descriptions.json', 'r') as f:\n",
                "    # Load JSON data as a dictionary\n",
                "    var_desc = json.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(df.loc[:, 'parentesco1'].unique())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Cleaning\n",
                "- There are a few features which are coded as strings but mix strings and integers/floats (see example below):\n",
                "    - 'dependency', \n",
                "    - 'edjefe', \n",
                "    - 'edjefa'\n",
                "    - 'tamviv'\n",
                "\n",
                "We will need to decide how to handle these."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lit review\n",
                "Below are notes from research papers we consulted. The focus on variables we may create and tips on how we may do our analysis.\n",
                "\n",
                "[Understanding the Determinants of Poverty](https://web.worldbank.org/archive/website01407/WEB/IMAGES/PMCH8.PDF)\n",
                "\n",
                "- using the highest level of the individuals in the household as the \n",
                "household level characteristic. IE, education level of the most highly educated\n",
                "person in the household\n",
                "\n",
                "[Introduction to Poverty Analysis](https://documents1.worldbank.org/curated/en/775871468331250546/pdf/902880WP0Box380okPovertyAnalysisEng.pdf)\n",
                "\n",
                "- p88 - use household head characteristics\n",
                "\n",
                "[HOUSEHOLD CHARACTERISTICS AND POVERTY: A LOGISTIC REGRESSION ANALYSIS](https://www.jstor.org/stable/23612271?seq=8)\n",
                "\n",
                "- p310\n",
                "    - use presence of disability, able-bodied persons, in the household\n",
                "    - sex ratio in household\n",
                "    - child/woman ratio in household\n",
                "    - proportion of female workers to total workers\n",
                "    - dependency ratio\n",
                "\n",
                "[Understanding poverty through household and individual level characteristics](https://worldbank.github.io/SARMD_guidelines/note-hhdchars.html)\n",
                "\n",
                "- \"For example, it is not true in general that female-headed households have lower levels of expenditures per capita\"\n",
                "- \"It is true, however, that urban households have significantly higher expenditures per capita\"\n",
                "\n",
                "[The DHS Wealth Index](https://dhsprogram.com/pubs/pdf/cr6/cr6.pdf)\n",
                "\n",
                "- \"For this reason, Filmer and Pritchett recommended using principal components analysis\n",
                "(PCA) to assign the indicator weights, the procedure that is used for the DHS wealth index.\"\n",
                "\n",
                "[Poverty and its measurement](https://www.ine.es/en/daco/daco42/sociales/pobreza_en.pdf)\n",
                "\n",
                "- p8-9 - calculate income per consumption unit rather than per capita\n",
                "\n",
                "[ARE POOR INDIVIDUALS MAINLY FOUND IN POOR HOUSEHOLDS? EVIDENCE USING NUTRITION DATA FOR AFRICA](https://www.nber.org/system/files/working_papers/w24047/w24047.pdf)\n",
                "\n",
                "[Moving from the Household to the Individual: Multidimensional Poverty Analysis](https://arxiv.org/ftp/arxiv/papers/1304/1304.5816.pdf)\n",
                "- \"Using longitudinal data Medeiros and Costa (2008) conclude that\n",
                "feminisation of poverty has not occurred in the eight Latin American countries they\n",
                "studied. Their findings are invariant to different measures and definitions of poverty.\"\n",
                "- \"marital status is an important consideration when discussing poverty incidence\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Exploration"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, we see that the Target variable is not consistently distributed. There are far more values of 4 than there are of 1, which may present issues for classification."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below, we ran a bivariate regression for each feature (except for 'dependency', 'edjefe', and 'edjefa' - see note about need for data cleaning above) on the Target, filtered out results not significnat at the 5% level and returned:\n",
                "\n",
                "* the top 5 features by r-squared\n",
                "    \n",
                "* the top 5 features by the absolute value of the coefficient"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features_to_include = [x for x in var_desc.keys() if x not in ['Id', 'idhogar', 'dependency', 'rez_esc', 'hh_head', 'parentesco1', 'hh_head_exists']]\n",
                "# CAN WE REMOVE DEPENDENCY FROM THIS LIST AFTER THE CLEANING?\n",
                "# WHY IS REZ_ESC ALL NA?\n",
                "\n",
                "\n",
                "df_subset = df[features_to_include]\n",
                "df_subset "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the variable data types\n",
                "df_subset.dtypes.value_counts() \n",
                "g = df.columns.to_series().groupby(df.dtypes).groups\n",
                "g"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_subset.isna().sum().sort_values()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace inifinity with NA\n",
                "#df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "\n",
                "# Fill in NaN with the column mean\n",
                "df_subset = df_subset.fillna(df_subset.mean())\n",
                "# df_subset = df.copy()\n",
                "\n",
                "# Select the target column and the other columns of interest & exclude the 3 columns discussed above with mixed datatypes \n",
                "target_col = 'Target'\n",
                "other_cols = [x for x in df_subset.columns if x not in ['Target']]\n",
                "results_df = pd.DataFrame(columns=['variable', 'coefficient', 'p_value', 'r_squared'])\n",
                "\n",
                "# Iterate over each independent variable in the dataframe\n",
                "for col in df_subset.columns[:-1]:\n",
                "    # Fit a linear regression model on the independent variable and target\n",
                "    X = df_subset[[col]]\n",
                "    y = df_subset[target_col]\n",
                "    X = sm.add_constant(X)\n",
                "    model = sm.OLS(y, X).fit()\n",
                "    \n",
                "    # Get the coefficient, p-value, and R-squared for the model\n",
                "    coeff = model.params[1]\n",
                "    p_value = model.pvalues[1]\n",
                "    r_squared = model.rsquared\n",
                "    \n",
                "    results_df.loc[len(results_df)] = [col, coeff, p_value, r_squared]\n",
                "\n",
                "results_df['variable_desc'] = results_df['variable'].map(var_desc)\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter results where p_value is less than or equal to 0.05 and sort by r-squared\n",
                "results_df = results_df[results_df['p_value'] <= 0.05]\n",
                "results_df = results_df.sort_values(by='r_squared', ascending = False)\n",
                "print(results_df.head(5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter results where p_value is less than or equal to 0.05 and sort by coefficient\n",
                "results_df = results_df[results_df['p_value'] <= 0.05]\n",
                "results_df = results_df.iloc[abs(results_df['coefficient']).argsort()[::-1]] \n",
                "print(results_df.head(5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df.to_csv('regression_results.csv', index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comparison of individual and household level characteristics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Individual = ['v18q', 'tamviv', 'escolari', 'dis', 'male', 'female', 'estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6',\\\n",
                "'estadocivil7','parentesco2','parentesco3','parentesco4', 'parentesco5', 'parentesco6', 'parentesco7',\\\n",
                "'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11' ,'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'mobilephone']\n",
                "# 'parentesco1', 'rez_esc',\n",
                "\n",
                "Household = [\n",
                "    'v2a1','hacdor',\n",
                "'rooms', 'hacapo','v14a', 'refrig', 'v18q1', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2','r4m3','r4t1', 'r4t2','r4t3','tamhog','hhsize',\n",
                "'paredblolad','paredzocalo','paredpreb','pareddes','paredmad','paredzinc','paredfibras','paredother','pisomoscer','pisocemento',\n",
                "'pisoother','pisonatur','pisonotiene','pisomadera','techozinc', 'techoentrepiso', 'techocane', 'techootro','cielorazo','abastaguadentro',\n",
                "'abastaguafuera','abastaguano', 'public','planpri','noelec','coopele','sanitario1','sanitario2','sanitario3','sanitario5','sanitario6','energcocinar1',\n",
                "'energcocinar2','energcocinar3','energcocinar4','elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6','epared1','epared2','epared3','etecho1',\n",
                "'etecho2','etecho3','eviv1','eviv2','eviv3',#'idhogar',\n",
                "'hogar_nin','hogar_adul','hogar_mayor','hogar_total', 'edjefe','edjefa', #'dependency',\n",
                "'meaneduc','bedrooms','overcrowding','tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5','computer','television','qmobilephone',\n",
                "'lugar1','lugar2','lugar3','lugar4','lugar5','lugar6',\n",
                "'area1','area2']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we split on these we learn the following:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_subset2 = df_subset.copy() \n",
                "df_subset = df_subset[Household].fillna(df_subset[Household].mean())\n",
                "\n",
                "# Select the target column and the other columns of interest\n",
                "target_col = \"Target\"\n",
                "other_cols = Household\n",
                "\n",
                "# Create an empty dataframe to store the regression results\n",
                "results_df = pd.DataFrame(columns=[\"variable\", \"coefficient\", \"p_value\", \"r_squared\"])\n",
                "\n",
                "# Iterate over each independent variable in the dataframe\n",
                "for col in df_subset.columns[:-1]:\n",
                "    # Fit a linear regression model on the independent variable and target\n",
                "    X = df_subset[[col]]\n",
                "    y = y_train[\"Target\"]\n",
                "    X = sm.add_constant(X)\n",
                "    model = sm.OLS(y, X).fit()\n",
                "\n",
                "    # Get the coefficient, p-value, and R-squared for the model\n",
                "    coeff = model.params[1]\n",
                "    p_value = model.pvalues[1]\n",
                "    r_squared = model.rsquared\n",
                "\n",
                "    # Add the results to the results dataframe\n",
                "    results_df.loc[len(results_df)] = [col, coeff, p_value, r_squared]\n",
                "\n",
                "# Add a column with the variable descriptions\n",
                "results_df[\"variable_desc\"] = results_df[\"variable\"].map(var_desc)\n",
                "\n",
                "# Filter the results where p_value is less than or equal to 0.05\n",
                "results_df = results_df[results_df[\"p_value\"] <= 0.05]\n",
                "\n",
                "# Sort the results by r-squared from least to greatest\n",
                "results_df = results_df.sort_values(by=\"r_squared\", ascending=False)\n",
                "\n",
                "# Print the results dataframe\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_subset = df_subset2.copy()\n",
                "df_subset = df_subset[Individual].fillna(df_subset[Individual].mean())\n",
                "\n",
                "# Select the target column and the other columns of interest\n",
                "target_col = \"Target\"\n",
                "other_cols = Individual \n",
                "\n",
                "# Create an empty dataframe to store the regression results\n",
                "results_df = pd.DataFrame(columns=[\"variable\", \"coefficient\", \"p_value\", \"r_squared\"])\n",
                "\n",
                "# Iterate over each independent variable in the dataframe\n",
                "for col in df_subset.columns[:-1]:\n",
                "    # Fit a linear regression model on the independent variable and target\n",
                "    X = df_subset[[col]]\n",
                "    y = y_train[\"Target\"]\n",
                "    X = sm.add_constant(X)\n",
                "    model = sm.OLS(y, X).fit()\n",
                "\n",
                "    # Get the coefficient, p-value, and R-squared for the model\n",
                "    coeff = model.params[1]\n",
                "    p_value = model.pvalues[1]\n",
                "    r_squared = model.rsquared\n",
                "\n",
                "    # Add the results to the results dataframe\n",
                "    results_df.loc[len(results_df)] = [col, coeff, p_value, r_squared]\n",
                "\n",
                "# Add a column with the variable descriptions\n",
                "results_df[\"variable_desc\"] = results_df[\"variable\"].map(var_desc)\n",
                "\n",
                "df_subset=df_subset2.copy()\n",
                "\n",
                "# Filter the results where p_value is less than or equal to 0.05\n",
                "results_df = results_df[results_df[\"p_value\"] <= 0.05]\n",
                "\n",
                "# Sort the results by r-squared from least to greatest\n",
                "results_df = results_df.sort_values(by=\"r_squared\", ascending=False)\n",
                "\n",
                "# Print the results dataframe\n",
                "print(results_df)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This demonstrates that on an individual level the Household characteristics provide a lot more explanitory value than the individual characteristics. Understanding that poverty does not exist in a vacuum-- the conditions of someones family and environment are important factors in understanding their financial status. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation of variables with Target\n",
                "no_target_df, _, target, _ = load_train_data()\n",
                "df = no_target_df.copy()\n",
                "df['Target'] = target\n",
                "corrs = abs(no_target_df.corrwith(df.loc[:,'Target'], method='spearman')).sort_values(ascending=False)\n",
                "corrs.head(10)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define functions to plot data\n",
                "def plot_continuous(df, continuous_col, target_col, cutoff=None):\n",
                "    \"\"\"\n",
                "    Plot target distribution by continuous variable\n",
                "    \n",
                "    Parameters:\n",
                "    df (pandas.DataFrame): Dataframe containing both continuous and target variables\n",
                "    continuous_col (str): Name of the continuous variable column\n",
                "    target_col (str): Name of the target variable column\n",
                "    cutoff (float): Optional cutoff value for the continuous variable\n",
                "    \n",
                "    Returns:\n",
                "    None\n",
                "    \"\"\"\n",
                "    df[continuous_col] = df[continuous_col].fillna(df[continuous_col].mean())\n",
                "    \n",
                "    # Cap continuous column at cutoff if specified\n",
                "    if cutoff:\n",
                "        df.loc[df[continuous_col] > cutoff, continuous_col] = cutoff\n",
                "    \n",
                "    # Define the bins using the distribution of the continuous variable\n",
                "    if cutoff:\n",
                "        bins = [df[continuous_col].min(), df[continuous_col].quantile(0.25), df[continuous_col].median(), df[continuous_col].quantile(0.75), cutoff]\n",
                "    else:\n",
                "        bins = np.percentile(df[continuous_col], [0, 25, 50, 75, 100])\n",
                "    bins = np.unique(bins)\n",
                "    # Create a label for each bin, handling duplicates if necessary\n",
                "    labels = []\n",
                "    for i in range(len(bins)-1):\n",
                "        label = f'{int(bins[i])}-{int(bins[i+1])}'\n",
                "        count = labels.count(label)\n",
                "        if count > 0:\n",
                "            label = f'{label} ({count})'\n",
                "        labels.append(label)\n",
                "    \n",
                "    df[f'{continuous_col}_bin'] = pd.cut(df[continuous_col], bins=bins, labels=labels, include_lowest=True, right=True, duplicates='drop')\n",
                "    \n",
                "    # Count target variable by continuous variable bin\n",
                "    target_by_bin = df.groupby(f'{continuous_col}_bin')[target_col].value_counts(normalize=True).unstack().fillna(0)\n",
                "    \n",
                "    # Define the x-axis tick labels\n",
                "    tick_labels = labels\n",
                "    \n",
                "    # Plot the bars\n",
                "    fig, ax = plt.subplots()\n",
                "    bar_width = 0.2\n",
                "    opacity = 0.8\n",
                "    \n",
                "    for i in range(len(target_by_bin.columns)):\n",
                "        rects = ax.bar(np.arange(len(tick_labels))+i*bar_width, target_by_bin.iloc[:,i], bar_width, alpha=opacity, label=f'Target = {target_by_bin.columns[i]}')\n",
                "    \n",
                "    # Add axis labels, title, and legend\n",
                "    ax.set_xlabel(continuous_col)\n",
                "    ax.set_ylabel('Percentage')\n",
                "    ax.set_title(f'Target Distribution by {continuous_col}')\n",
                "    ax.set_xticks(np.arange(len(tick_labels))+0.5*(len(target_by_bin.columns)-1)*bar_width)\n",
                "    ax.set_xticklabels(tick_labels)\n",
                "    ax.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "def plot_grouped_bar_chart(df, binary_col, target_col):\n",
                "    \"\"\"\n",
                "    Plots a grouped bar chart showing the percentage of each target category for each value of a binary column.\n",
                "\n",
                "    Parameters:\n",
                "    df (pandas.DataFrame): The DataFrame containing the data.\n",
                "    binary_col (str): The name of the binary column.\n",
                "    target_col (str): The name of the target column.\n",
                "\n",
                "    Returns:\n",
                "    None\n",
                "    \"\"\"\n",
                "\n",
                "    # Create a grouped bar chart\n",
                "    fig, ax = plt.subplots()\n",
                "    bar_width = 0.35\n",
                "    opacity = 0.8\n",
                "\n",
                "    # Get the unique values of the binary column\n",
                "    index = df[binary_col].unique()\n",
                "\n",
                "    # Count target variable by binary group\n",
                "    binary_yes = df[df[binary_col] == 1][target_col].value_counts().sort_index().tolist()\n",
                "    binary_no = df[df[binary_col] == 0][target_col].value_counts().sort_index().tolist()\n",
                "\n",
                "    # Calculate percentage of total for each target category within each binary group\n",
                "    binary_yes_perc = [count/sum(binary_yes) * 100 for count in binary_yes]\n",
                "    binary_no_perc = [count/sum(binary_no) * 100 for count in binary_no]\n",
                "    binary_yes_target_perc = [binary_yes[i]/(binary_yes[i]+binary_no[i]) * 100 for i in range(len(binary_yes))]\n",
                "    binary_no_target_perc = [binary_no[i]/(binary_yes[i]+binary_no[i]) * 100 for i in range(len(binary_no))]\n",
                "\n",
                "    # Define the x-axis tick labels\n",
                "    tick_labels = sorted(df[target_col].unique())\n",
                "\n",
                "    # Plot the bars\n",
                "    rects1 = ax.bar([i - bar_width/2 for i in range(len(tick_labels))], binary_yes_target_perc, bar_width, alpha=opacity, color='b', label=f'{binary_col}=Yes')\n",
                "    rects2 = ax.bar([i + bar_width/2 for i in range(len(tick_labels))], binary_no_target_perc, bar_width, alpha=opacity, color='orange', label=f'{binary_col}=No')\n",
                "\n",
                "    # Add axis labels, title, and legend\n",
                "    ax.set_xlabel(target_col)\n",
                "    ax.set_ylabel('Percentage of Total')\n",
                "    ax.set_title(f'{target_col} count by {binary_col}')\n",
                "    ax.set_xticks([i for i in range(len(tick_labels))])\n",
                "    ax.set_xticklabels(tick_labels)\n",
                "    ax.legend()\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot variables with the highest correlation with Target and other variables of interest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df, 'meaneduc', 'Target', cutoff=21)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df, 'SQBmeaned', 'Target', cutoff=441)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df,'v2a1_log','Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df,'SQBdependency','Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df,'escolari','Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_continuous(df,'hogar_nin','Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_grouped_bar_chart(df, 'cielorazo', 'Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_grouped_bar_chart(df, 'eviv3', 'Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_grouped_bar_chart(df,'epared2','Target')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_grouped_bar_chart(df,'pisocemento','Target')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Generally it seems easy to classify the relatively wealthy, and hard to classify the relatively poor, even with the highest correlation variables"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
